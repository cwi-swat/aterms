
%{{{ Introduction

\section{Introduction}
Modern software systems consist of components which often
interact with each other using a so-called software coordination
architecture.
The data exchanged and processed by these components may be very diverse.
We developed a common datatype, called \emph{\ATerm},
to be used for the exchange of information
between components but which can also be directly processed by them.

Given the fact that we are working on applications in the area of interactive
programming environments \cite{Kli93.meta,BKMO97}, 
such a common datatype is of great convenience
when exchanging data. Typical examples of these applications include
parsers, compilers, typecheckers, and syntax directed editors.
Typical data being exchanged between and processed by these applications
are (abstract) syntax trees.
The \ATerm\ datatype is very close to this kind of
datatype and it is therefore very natural to use \ATerms\ for internal 
representation also. By providing a well-defined set of functions, it is possible
to perform operations on the \ATerms\ without first converting them to some 
component specific representation.

The applications we develop vary from
development tools for domain specific languages \cite{DK98} 
to factories for  renovation of COBOL programs \cite{BSV97}. 
The \ATerm\ datatype is also the basic datatype to represent the terms
manipulated by the rewrite engines generated by the \asdf2C compiler
\cite{BKO99}.
Because of the size of the problems that these applications
have to handle, we need a very efficient implementation of \ATerms,
both in time and space.

We will start by describing the intended application domain of
the \ATerm\ datatype in Section \ref{app_domain}. 
After a definition of both the abstract
and concrete syntax of \ATerms\ and a description of the most
important operations defined on them, 
we will present a number of requirements on the
implementation of this datatype in Section \ref{requirements}.
In Section \ref{design} we will discuss the design of our implementation
followed by an in-depth description of some implementation issues 
in Section \ref{implementation}.

%}}}
%{{{ The application domain

\section{ATerm Functionality}
\label{app_domain}

The applications for which the \ATerm\ datatype is intended are mainly
language processing tools, such as programming
environments and program transformation tools.
These tools mainly operate on (abstract) syntax trees where
each node of a tree consists of a function symbol and zero or 
more arguments (descendants). 
The \ATerm\ datatype is also used to represent terms that are
rewritten by rewrite engines such as interpreters and 
compilers \cite{BKO99}.
Both types of applications, representation of (abstract) syntax trees
and terms in rewrite engines, require an efficient representation
of function applications. 

Besides these function applications, a small number of other subtypes
are allowed to make the \ATerm\ datatype more generally applicable.
These subtypes include integer constants, real number constants,
binary large data objects (blobs), lists of \ATerms, and the special
placeholder subtype that is used to represent typed gaps in \ATerms.

The fact that the \ATerm\ datatype is used within the components
of language processing tools demands extra functionality with respect
to storage facilities of auxiliary non-structural information derived by these
components, e.g., a parser may add coordinates to the nodes
describing the actual yield of the node in the text and a formatter
may add font and/or color information used by an editor when displaying
the textual representation of the tree.
So, an \ATerm\ can be extended with a list of \emph{annotations}
(hence the name \emph{Annotated Terms}), that is used
to add extra information to it.

%}}}
%{{{ The ATerm datatype

\subsection{The \ATerm\ Datatype}
\label{aterm}

We describe the datatype called \emph{annotated term} (or \emph{ATerm})
in more detail.
The abstract definition of this datatype is as follows.
\begin{itemize}
\item An integer constant is an \ATerm.
\item A real constant is an \ATerm.
\item A list of zero or more \ATerms\ is an \ATerm.
\item A function application consisting of a function symbol and
      zero or more \ATerms\ (arguments) is an \ATerm.
      The number of arguments of the function is called
      the \emph{arity} of the function.
\item A blob (Binary Large data OBject) containing a length indication
      and a byte array of binary data (possibly very large)
      is an \ATerm.
\item A placeholder term containing an \ATerm\ representing the
      type of the placeholder is an \ATerm.
\item A list of \ATerm\ pairs may be associated with every \ATerm\
      representing a list of $(label,annotation)$ pairs.
\end{itemize}
Each of these constructs except the last one can be seen as a subtype of
the generic type {\tt ATerm}. The last construct is the \emph{annotation
construct}, which makes it possible to annotate terms with transparent
information\footnote{Transparent in the sense that the result of
most operations is independent of the annotations. This makes it easy
to completely ignore annotations. Typical examples of the use of annotations
include annotating parse trees with positional or typesetting information, 
and annotating abstract syntax trees with the results of typechecking.}.

Appendix \ref{concrete-syntax} contains a formal definition 
of the concrete syntax of \ATerms. The primary reason for having a concrete
syntax is to be able to exchange \ATerms\ in an accessible, humanly 
readable form. In Section \ref{implementation} we also discuss a compact 
binary format for the exchange of \ATerms\ in a format that is only 
suitable for processing by machine.
We will only give a number of examples to show some of the features of the 
textual representation of \ATerms.

\begin{itemize}
\item Integer and real constants are written down as-is:\newline
	{\tt 1, 3.14, -0.7E34} are all valid \ATerms.
\item Function applications are represented using a function name followed
      by an open brace, the arguments of the function application separated
      by commas, and is terminated by a closing brace. When there are no
      arguments, the braces can be omitted. Typical examples are:\newline
      {\tt f(a,b), "test!"(1,2.1,"Hello world!")}.\newline
      These examples show that double quotes can be used to delimit
      function applications that are not identifiers.
\item Lists are represented using an rectangular open brace, a number of
      list elements separated by commas and a terminating rectangular closing
      brace:\newline
      {\tt [1,2,"abc"], [], [f,g([1,2]),x]}.
\item A placeholder is represented using an opening angular bracket,
      followed by a subterm and a closing angular bracket:\newline
      {\tt <int>, <[3]>, <f(<int>,<real>)>}.
\item Blobs do not have a concrete syntax because their humanly readable form
      depends on the actual blob content.
\end{itemize}

\subsection{Constants and Operations}
\label{operations}

Six constants are defined to represent the different subtypes in \ATerms. 
\begin{itemize}
\item {\tt INT}:  An integer term.
\item {\tt REAL}: A real term.
\item {\tt LIST}: A list of terms.
\item {\tt APPL}: A function application.
\item {\tt PLACEHOLDER}: A placeholder term.
\item {\tt BLOB}: A term containing binary data.
\end{itemize}
These constants are needed when determining the type of an arbitrary \ATerm.

The \ATerm\ datatype offers a number of operations to manipulate terms.
They take care of reading and writing terms,
there is a predicate to check the equality of two terms,
two functions {\tt make} and {\tt match} construct a term
and to take a term apart, respectively.
Both functions use a string pattern containing placeholders or gaps.
These holes determine the places where \ATerms\ may be substituted.
An example of a pattern is ``{\tt and(<int>,<appl>)}''.
Patterns are in fact comparable to the format strings in the print statements
in C.
There are also three functions to manipulate annotations.
Finally, there exists a function to obtain the type of a term.

The operations are as follows:
\begin{itemize}

\item {\tt ATerm readFromString(\emph{string})}: Creates a new term by parsing
\emph{string}. When a parse error occurs, a message is printed, and
a special error value is returned.
\item {\tt ATerm readFromTextFile(\emph{file})}: Creates a new term by 
parsing the data from \emph{file}. Again, parse errors result in a 
message being printed and an error value being returned.
\item {\tt ATerm readFromBinaryFile(\emph{file})}: Creates a new term by reading a
compact, portable, binary representation from \emph{file}. This format
is discussed in Section \ref{baf}.

\item {\tt Boolean writeToTextFile(\emph{term}, \emph{file})}: Write the text
representation of \emph{term} to \emph{file}. Returns {\tt true} for
success and {\tt false} for failure.
\item {\tt Boolean writeToBinaryFile(\emph{term}, \emph{file})}: Write the compact, portable, binary representation of \emph{term} to \emph{file}. Returns
{\tt true} for success, and {\tt false} for failure (see Section \ref{baf}).
\item {\tt String writeToString(\emph{term})}: Return the text representation
of \emph{term} as a string.

\item {\tt Boolean ATisEqual(\emph{t1},\emph{t2})}: Check if two terms are 
equal. The annotations of {\emph t1} and {\emph t2} must also be equal.

\item {\tt ATerm make(\emph{pattern}, \emph{args})}: This operation returns a new
term created by taking the string pattern, parsing it as a term and filling
 the gaps in the resulting term with values taken from {\tt args}.
If the parse fails, a message is printed and the program is aborted.

\item {\tt ATermList match(\emph{term}, \emph{pattern})}: Match \emph{term} against
\emph{pattern}, and return a list of substitutions for the placeholders
in \emph{pattern}.
If the parse fails, a message is printed and the program is aborted.
If the term itself contains placeholders these may occur in the 
resulting substitutions.
If the match fails, a special error value is returned.

\item {\tt ATerm setAnnotation(\emph{term}, \emph{label}, \emph{annotation})}:
Return a copy of \emph{term} with annotion labeled by \emph{label} changed
to \emph{annotation}. If \emph{term} does not have an annotation with
the specified label, it is added.

\item {\tt ATerm getAnnotation(\emph{term}, \emph{label})}:
Retrieve the annotation labeled by \emph{label} of \emph{term}.
If \emph{term} does not have an annotation with the specified label,
a special error value is returned.

\item {\tt ATerm removeAnnotation(\emph{term}, \emph{label})}: Return a copy
of \emph{term} with its annotation labeled by \emph{label} removed.
If \emph{term} does not have an annotation with the specified label,
it is returned unchanged.

\item {\tt Integer getType(\emph{term})}: Retrieve the type of a term. This
operation returns one of the constants mentioned above.
\end{itemize}
Although this may seem a rather restricted set of functions, it
is sufficiently powerful for most users to build simple applications
with the \ATerm\ library.
The set of operations is simple enough to learn in a relatively
short period of time. We refer to this interface as the \emph{level one}
interface of the \ATerm\ datatype.
 
In order to accomodate ``advanced users'' of \ATerms\
we also provide a \emph{level two} interface, 
which provides a more sophisticated
set of datatypes and functions. These extensions are useful only
when more control over the underlying implementation is needed
or in situations where some operations that can be implemented using
level one constructs, can be expressed more concisely and implemented more
efficiently using level two constructs.
The level two interface is a strict superset of the level one
interface. A detailed description of the level two interface
can be found in Appendix \ref{level2}.

%}}}
%{{{ Requirements

\section{Requirements on the Implementation}
\label{requirements}

The \ATerm\ library is intended for use in tools like programming environments and
rewriting engines that must process large terms in an efficient way.
For instance, (abstract) syntax trees contain a lot of redundant 
information which can easily 
be shared to reduce their size.
In this section we will formulate a number of constraints
that play an important role in the design and implementation of the
\ATerm\ datatype.

The applications we have in mind must be able to handle
a large number of terms as well as huge terms. We have applications
that need to handle tens of millions of terms.
This observation makes it possible to
formulate a number of efficiency requirements on the implementation of
the \ATerm\ datatype:

\begin{itemize}
\item The memory required to store a large number of \ATerms\ in main memory
should be as small as possible. 
\item The operations defined on \ATerms\ must be implemented very
efficiently, both in time and space.
\item External storage of \ATerms\ should be very cheap.
\item It should be possible to exchange \ATerms\ between processes
      (possibly running on different processors or even different machines)
      without having to transfer large amounts of data, and without 
      needing excessive amounts of CPU time for (de)compression.
\end{itemize}

We also present a couple of requirements to ensure that the implementation
of the \ATerm\ datatype is easy to use:
\begin{itemize}
\item Automatic garbage collection is very important so \ATerm\ users
      do not need to deallocate \ATerm\ objects explicitly.
\item The interface of the library should
      be intuitive and as small as possible to avoid a steep
      learning curve. This is among others realized by restricting the 
      level one
      interface to the thirteen operations presented in 
      Section \ref{operations}.
\end{itemize}

Besides these requirements there are a number of practical 
issues to consider that have a great impact on the design,
and that make this a fairly unique problem:
\begin{enumerate}
\item Most applications exhibit a high level of reduncancy in the terms
      being processed. Large terms often have a significant number of
      identical subterms. Intuitively this can be explained from the
      fact most applications process terms with a fixed signature and
      a limited tree depth. When the amount of terms that is being processed
      increases, it is plausible that the similarity between terms also
      increases.
\item In typical applications less than 0.1 percent of all terms have
      an arity that is higher than 5.
\item The expected lifetime of terms in most applications is very short.
      This means that garbage collection must be fast and should touch
      as few memory locations as possible to improve caching and paging 
      performance.
\item The total memory requirements of an application cannot be estimated
      in advance. It must be possible to allocate more memory incrementally.
%%\item A lot of applications will use annotations only sparingly. It is
%%      therefore important that annotations do not add significantly to the
%%      overall memory requirements when they are not used.
\item In order to have a portable yet efficient implementation, the
      implementation language will be C. This poses some special 
      requirements for the garbage collection strategy\footnote{We also
      implemented the library in Java. In this case, a lot of the issues
      we discuss in this paper are not relevant, either because we can use 
      built-in features of Java (garbage collection), or
      because we just cannot express these low level concerns in Java.}.
\end{enumerate}
     
%}}}
%{{{ Design

\section{Design}
\label{design}

In this section, we discuss a number of design decisions we made,
all of which are based on the requirements presented in the
previous section. These decisions involve the use of maximal sharing,
the garbage collection strategy, and the representation of lists.

\subsection{Maximal Sharing}
\label{hashing}
Probably the most important design decision is to base our implementation
of the \ATerm\ datatype on \emph{maximal sharing}. In order to fully exploit
the redundancy that is typically present in terms that are built,
we never create the same term twice. Before a new term is created,
a lookup is done to see if that term already exists. If it exists, the old
version is reused and no new term is created.

This design decision is a typical example of a space/time trade-off,
some time is invested at term creation time in order to use 
significantly less space. In this case however, part of invested
time can be reclaimed later on because the equality check on maximally
shared terms is very cheap. To check the equality of 'unshared'
terms, all nodes must be compared in the worst case. 
In the maximally shared case however
two terms are only equal when they are in fact the same term, so 
comparing their memory addresses suffices to check for equality. The
complexity of the equality check has effectively been reduced from
$O(n)$ to $O(1)$. Depending on the application, this gain easily
offset the time invested to build maximally shared terms.

Another consequence of our approach is less fortunate:
Because terms can be shared without the creator knowing it,
terms cannot be modified without creating unwanted side-effects.
This means that terms effectively become \emph{immutable} after creation.
Destructive updates on maximally shared terms are not allowed, except maybe
in special situations where it is known in advance that a term is not
shared. Especially in list operations, this immutability can be
expensive. It is often the responsibility of the user of the library
to choose algorithms that minimize the effect of this shortcoming.

The maximal sharing of terms can only be maintained when we check
at term creation time whether a particular term already exists. The
lookup must be \emph{extremely fast}, in order to ensure efficient term
creation. We use the fastest technique available
in this situation, namely hashing. Using a hash function that depends on the
\emph{addresses} of the function symbol and the arguments of a function
application, we can quickly search the term database to find a function 
application before creating it.

\subsubsection{Collisions}
One of the issues when using hash techniques
is how to handle collisions in the hash table. The simplest
technique is to linearly chain entries together that hash to the 
same bucket. In this scenario, one pointer is needed in each
object for hash chaining, which in our case means a memory
overhead of about 25 percent. Other solutions for collision
resolution will either increase the memory requirements (e.g.
binary tree chaining), or the time needed for insertions
or deletions (open addressing, see \cite{Knuth73}).
We therefore use linear hash chaining in our implementation. 

\subsubsection{Direct or Indirect Hashing}
Another issue is whether to store all terms directly in
the hash table, or to only store references in it.
Storing the objects directly in the hash table has the advantage
that we can save a memory access when retrieving a term.
However, there are severe drawbacks to this approach:
\begin{itemize}
\item We cannot rehash the old terms because rehashing
      means that we have to physically move the objects in memory.
      In a setting where we use C as an implementation language, moving
      objects in memory is not allowed because we can only determine
      a conservative root set and therefore are not allowed to change
      the pointers to roots.
      This would mean that the hash table could not grow beyond its 
      initial size, clearly an undesirable feature.
\item The internal fragmentation is increased, because empty slots
      in the hash table are as large as the object size instead of
      only one machine word.
\item We would need a separate hash table for each term size in order to
      decrease the internal fragmentation. 
\end{itemize}

Because of these problems, we decided to use linear hash chaining
in combination with indirect hashing.
When the load of the hash table reaches a certain threshold, 
we allocate a larger table, rehash all the entries of the 
old table in the new one, and free the old hash table.

%{{{ Garbage collection

\subsection{Garbage Collection}
The most common strategies for automatic recycling of unused space
are reference counting, mark-compact collection, and 
mark-sweep collection. We discuss these three strategies 
in turn, and explain why we base our design on mark-sweep garbage collection.

\subsubsection{Reference Counting}
Reference counting in its simplest form works by keeping track of the 
number of references to an object. If this number drops to zero, 
there are no more references to it and it can be reclaimed.

Reference counting has the advantage that unused terms can be reclaimed 
immediately when they become unused, while in the other strategies 
reclamation of space is more batch oriented. 
In most of the target applications this is not a major
issue but it might be important in some cases, especially
in real-time applications where the unpredictable pauses in execution
caused by sweeping garbage collection are unacceptable.
The usual problem with reference counting that circulair
structures are not collected does not apply because terms cannot
be circulair.

Unfortunately, in languages like C that do not offer much runtime support
for dynamic memory management, the programmer needs to assist
in updating reference counts, for instance at function exit.
Maybe an even bigger disadvantage is that reference counts need to
be updated every time the number of references to an object changes.
The total amount of work that has to be done for a pure reference counting
approach is much higher than for the other strategies.

Another disadvantage of reference counting is the space overhead needed 
to store the reference count.
As we shall see later on, it is possible to implement the \ATerm\ datatype
using only a couple of machine words\footnote{We assume a word size
of 4 bytes in this paper.} per object. Adding a whole word
for keeping track of the reference count is therefore unacceptable.

\subsubsection{Mark-compact Garbage Collection}

Mark-compact garbage collection works by periodically copying all
\emph{live} objects to a fresh empty memory space, and freeing the
original space. Although this strategy has merits, it is unusable in 
our situation. 
Mark-compact garbage collection assumes that objects
can be relocated to a different memory location, requiring that
all references to objects can be updated.
But when using C as an implementation language, we cannot
positively identify \emph{all} references to an object without complex
support from the programmer. This problem can only be solved by
using one more level of indirection but this solution is too expensive.

\subsubsection{Mark-sweep Garbage Collection}

Mark-sweep garbage collection works using three (sometimes two) phases.
In the first phase, all objects on the heap are marked as `dead'.
In the second phase, all objects reachable from the known set of root
objects are marked as `live'. In the third phase, all `dead' objects
are swept into a list of free objects.

Mark-sweep garbage collection is very attractive, because it
can be implemented in C, efficiently and without support from the 
programmer or compiler \cite{BW88,Bo93}. 
Mark-sweep collection is more efficient,
both in time and space than reference counting \cite{JL96}. A possible
drawback is increased memory fragmentation compared to for instance
mark-compact collection.
The typical space overhead for a mark-sweep garbage collection algorithm is
only 1 bit per object, whereas a reference count field would take at
least three or four bytes.

\subsubsection{Reusing an Existing Garbage Collector}
A number of excellent generic garbage collectors for C are 
freely available, so a legitimate question is why not reuse an
existing implementation instead of implementing our own?

We have examined a number of alternatives, but none of these quite 
fitted our needs. The Boehm-Weiser garbage collector \cite{BW88} came close,
but we face a number of unusual circumstances that render existing
garbage collectors impractical:

\begin{itemize}
\item The term database (hash table) always contains references to 
      all objects. It must be possible to instruct the garbage collector
      not to scan this area for roots.
\item After an object becomes garbage, it must also be removed from the
      term database. This means that we need very low level control
      over the garbage collector.
\item The \ATerm\ datatype has some special characteristics that can
      be exploited to dramatically increase performance:
      \begin{itemize}
	\item Destructive updates are not allowed. In garbage collection
              terminology, this means that there are no pointers from
              old objects to younger objects. Although we do not exploit
	      it in the current implementation, this characteristic
              makes the use of a \emph{generational} garbage collector
              very attractive.
        \item The overwhelming majority of objects only consists of 
              between 8 and 16 bytes.
	\item Practical experience has shown that not many root pointers
              are kept in static variables or on the generic C heap. 
              Performance can be increased
              dramatically if we eliminate the expensive scan through the
              heap and the static data area for root pointers. 
              The only downside is that we require
              the programmer to explicitly supply the set of roots
              that is located on the heap or in static variables.
      \end{itemize}
\end{itemize}
These observations allow us to gain efficiency on several
levels, using everything from low level system 'hacks' to high-level 
optimizations.

\subsubsection{The Mark-sweep Algorithm}
Given these facts, we opted for a straightforward version of mark-sweep 
garbage collection because this gives very good
performance in combination with a high level of maintainability of the
code that uses the library.

As we shall see in Section \ref{implementation}, 
most objects consist of only a couple of machine words. 
By restricting the maximum arity of a function, we can also
set an upper bound on the maximum size of objects.
This enables us to base the memory management 
algorithms we use on a small number of block sizes.

%}}}

%{{{ Representation of lists

\subsection{Representation of Lists}
After the function application, the list construct is the second most 
used \ATerm\ construct. A (memory) efficient representation of lists is 
therefore very important. 
Due to the nature of the operations on \ATerm\ lists, there are
two obvious list representations: an array of term references or a linked
list of term references. Experiments have shown that in typical applications
quite varying list sizes are encountered. This renders the array approach 
inferior, because adding and deleting elements of a list would become to
expensive. Consequently, we have opted for the linked list approach.
Lists are constructed using binary list constructors, containing 
a reference to the first element in the list and to the tail of the list.
This makes it very easy to perform the most commonly used operations
on list, namely adding or removing the first element of a list.

Other operations are more expensive, due to the fact that we cannot
allow destructive updates. Adding an element to the tail of a list
for instance, requires $n$ list creation operations, where $n$ is
the number of elements in the newly created list.

%}}}

%}}}
%{{{ Implementation

\section{Implementation}
\label{implementation}

In this section, we will describe some of the more interesting
implementation aspects of the C implementation of the \ATerm\
datatype. We will first present the memory efficient encoding
of \ATerms\ that we have chosen. We will explain how term sharing and
garbage collection can be implemented efficiently using this encoding.
Furthermore, we will describe how the different
operations on \ATerms\ described in Section \ref{aterm}
can be implemented efficiently.

%}}}
%{{{ Term encoding

\subsection{Term Encoding}

After discussing the requirements of the library (Section \ref{requirements}),
and the design (Section \ref{design}), we will now describe the implementation
of the \ATerm\ datatype in C \cite{KR88}

{\tt <PO> need a reference to ANSI-C!}.

An important issue in the implementation is how to represent \ATerms\ 
in such a way that all operations can be performed efficiently, without using
more memory than is absolutely necessary. 
We will first introduce the memory efficient encoding of \ATerms\ we have 
chosen. Then will discuss how this encoding can be used to implement all
operations efficiently, including term sharing and garbage collection.

We assume that one machine word consists
of 4 bytes, which is typical for modern architectures.
Every \ATerm\ object is stored in two or more machine words.
The first byte of the first word is called the \emph{header}
of the object, and consists of three fields (see Figure \ref{header}):

\begin{figure}[htb]
  \centerline{\epsfig{file=header.ps,scale=0.6}}
  \caption{\label{header}The header layout}
\end{figure}
 
\begin{itemize}
\item A field consisting of 1 bit used as a mark flag by the garbage
      collector.
\item A field consisting of 1 bit indicating whether or not this term
      has an annotation or not.
\item A field consisting of 3 bits that indicate the type of the term.
\item A field consisting of 3 bits representing the arity (number of
      pointers to other terms) of this object. When this field
      contains the maximum value of 7, the term must be a function application
      and the actual arity can be found retrieving the arity of the
      function symbol, which will be discussed later.
\end{itemize}
The rest of this word contains either the function symbol, length, or
nothing depending on the type of the node.
This will be discussed in more detail below.
The second word is always used for hashing, 
and links together all terms in the same
hash bucket.

The type of the \ATerm\ determines the exact layout and contents of
the node.
Figure \ref{encoding} shows the encoding of the different term types.\
In the following paragraphs the encodings for the term types 
are described in more detail.

\begin{figure}[!htb]
  \centerline{\epsfig{file=encoding.ps,scale=0.6}}
  \caption{\label{encoding}Encoding of the different term types}
\end{figure}

\paragraph{APPL encoding}
The remaining 3 bytes following the header in the first word are used to
represent the function symbol. The words following the second word contain
references to the function arguments.
In this way, function applications can be encoded in
$2+n$ machine words, with $n$ the arity of the function application.

\paragraph{LIST encoding}
The binary list constructor can be seen as a special function
application with no function symbol and an arity of 2. The
third word points to the first element in the list, this is called
the {\tt first} field, the fourth word
points to the remainder of the list, and is called the {\tt next} field. 
The length of the list is
stored in the three bytes after the header in the first word.
The empty list\footnote{Due to the uniqueness of terms, only
one instance of the empty list is present at any time.}
is represented using a LIST object with empty
first and next fields, and a length of 0.

\paragraph{INT encoding}
In an integer term, the third word contains the integer value.
The arity of an integer term is 0.

\paragraph{REAL encoding}
In an real term, the third and fourth word contain the real value 
represented by an 8 byte floating point number.
The arity of a real term is 0.

\paragraph{PLACEHOLDER encoding}
The placeholder term has an arity of 1, where the third
word contains a pointer to the placeholder type.

\paragraph{BLOB encoding}
The length of the data contained in a BLOB term is stored in
the three bytes after the header. A pointer to the actual data
is stored in the third word.

\subsection{Implementation of Term Sharing}

Our strategy to minimize memory usage is simple but
effective: we only create terms that are \emph{new}, i.e., that do not
exist already.  If a term to be constructed already exists, that term
is reused thus ensuring maximal sharing.  This strategy fully exploits
the redundancy that is typically present in the terms to be build.
The library functions to construct terms 
take care of building shared terms whenever possible.  The sharing of
terms is invisible.

Maximal sharing of terms can only be maintained when we check at every
term creation whether a particular term already exists or not. This
check implies a search through all existing terms but must nonetheless
be executed {\em extremely fast} in order not to impose an
unacceptable penalty on term creation.  Using a hash function that
depends on the internal code of the function symbol and the addresses
of its arguments, we
can quickly search for a function application before creating it.  
The terms are stored in a hash table.
The (modest but not negligible) costs at term creation time are hence
one hash table lookup.

Fortunately, we get two returns on this investment.  First, the
considerably reduced memory usage also leads to reduced (real-time)
execution time.  Second, we gain substantially since the equality
check on terms ({\tt term\_equal}) becomes very cheap: it reduces from
an operation that is linear in the number of subterms to be compared
to a constant operation (pointer equality).

The hash table does not contain the terms themselves, but pointers to the
terms. This provides a flexible mechanism of resizing of the table and
ensures that all entries in the table are of the same size.

\begin{itemize}
\item Resizing.
\item Relation with ``user-defined'' hash tables.
\end{itemize}

\subsection{Implementation of the Garbage Collector}

We have opted for a version of the make-sweep garbage collector.
Every object contains a single bit used by the mark-sweep algorithm
to indicate 'live' (marked) objects. At the start of a garbage collection cycle,
all objects are unmarked. The garbage collector tries to locate and mark all
live objects by traversing all terms that are explicitly protected by the
programmer (using the {\tt ATprotect} function), and by scanning the stack 
looking for words that
could be references to objects. When such a word is found, the object 
(and the transitive closure of all of the objects it refers to) are marked 
as 'live'. 

This scan of the stack causes all objects referenced from local
variables to be protected from being garbage collected.
Our garbage collector 
is a  conservative collector in the sense that some of the words on the
stack could accidentally look like object references, resulting in objects 
marked as 'live' that are actually garbage.
When all live objects are marked, a single sweep through the heap
is used to store all objects that are free in separate lists of free
objects, one list for each object size.

Allocation of objects is now simply a matter of taking the first element
from the appropriate free-list, which is an extremely cheap operation.
If garbage collection does not yield enough free objects, new memory blocks
will be allocated to satisfy allocation requests. 


%}}}
%{{{ The Binary ATerm Format (BAF)

\subsection{The Binary \ATerm\ Format (BAF)}
\label{baf}
As discussed in the introduction, the efficient exchange of \ATerms\ 
between processes is very important. The simplest form of exchange 
is the concrete syntax presented in Section \ref{concrete-syntax}.
This would involve pretty printing the term on one side, and parsing it
on the other. The concrete syntax is not a very efficient exchange format
however, because function symbol and subterm sharing cannot be expressed
this way.

A better solution would be to exchange a representation in which 
sharing (both of function symbols and subterms) can be expressed
concisely
A memory dump is not practical, because addresses
in the address space of one process have no meaning in the address space
of another process.

In order to address these problems, we have developed \baf, the
{\bf B}inary {\bf A}Term {\bf F}ormat.
Instead of writing addresses, we assign a unique number
to each subterm and each symbol occuring in a term that we want to 
exchange. When referring to this term, we could use this number instead
of its address. 

When writing a term, we first start by writing all function symbols
used in this term. Each function symbol consists of the string 
representation of the symbol followed by its arity.

To write a function application, first the index of the
function symbol is written. Then the indices of the arguments
are written. When an argument consists of a term that has not
been written yet, the index of the argument is first written itself
before continuing with the next argument.
In this way, every subterm is written exactly once. Every time
a parent term wishes to refer to a subterm, it just uses the
index.

\subsubsection{Exploiting \ATerm\ regularities}
When sending a large term containing a lot of subterms, the subterm
indices can become quite large. Consequently a lot of bits are needed
to represent these indices. We can considerably reduce the size of these
indices when we take into account some of the regularities in the
structure of terms. Empirical study shows that the set of function symbols
that actually occur at each argument position of all terms over a
certain function symbol is often very small. A rationale for this is that
although \ATerm\ applications themselves are not typed, the datatypes
they represent often are. In this case, function applications represent
objects and the type of the object is represented by the function
symbol. The type hierarchy determines which types can occur at each
position in the object.

We exploit this knowledge by grouping all terms occording to their
top function symbol. Terms that are not function applications are
grouped based on dummy function symbols, one for each term type.
For each function symbol, we determine which function symbols can
occur at each argument position. When writing all function symbols
at the start of the BAF file, we also write this information also.

When writing the argument of a function application, we start
by writing the actual symbol of the argument. Because this symbol
is taken from a limited set of function symbols (only those symbols
that can accually occur at this position), we can use a very small
number to represent this it. Following this function symbol we
write the index of the argument term itself in the table of terms
over this function symbol instead of the index of the argument in
the total term table.

\subsection{Example}
As an example, we show how the term {\tt mult(s(s(z),s(z)))} is represented
in \baf. This term contains three function symbols: {\tt mult} with
arity two, {\tt s} with arity one, and {\tt z} with arity zero.
When grouping the subterms by function symbol we get:
\vspace{0.5cm}

\begin{tabular}{|c|c|c|}
\hline
mult               & s & z       \\
\hline
mult(s(s(z)),s(z)) & s(s(z)) & z \\
                   & s(z)    &   \\
\hline
\end{tabular}
\vspace{0.5cm}

When we look at the function symbols that can occur
at every argument position we get:
\vspace{0.5cm}

\begin{tabular}{|c|c|c|c|}
\hline
position & mult               & s    & z \\
\hline
0        & s                  & s, z & \\
1        & s                  &      & \\
\hline
\end{tabular}
\vspace{0.5cm}

The information in this last table is written first in a 
straightforward manner
that we will not discuss here. It is important to note that
in most cases the number of function symbols is very small
compared to the number of terms that is to be written. Storing
some extra information for every function symbol in order
to better compress the actual term data is no problem.

The following bits need to be written in order to encode
the example term:

\begin{tabbing}
00 \= : \= \kill
   \> : \> No bits need to be written to identify the function symbol {\tt s},\\
   \>   \> because it is the only possible function symbol at the first \\
   \>   \> argument position of {\tt mult}. \\
0  \> : \> One bit indicates which term over the function symbol {\tt s} is \\
   \>   \> written ({\tt s(s(z))}). Because this term has not been written yet,\\
   \>   \> it is done so now. \\
0  \> : \> The function symbol of the only argument of {\tt s(s(z))} is {\tt s}.\\
1  \> : \> {\tt s(z)} is has index 1 in the termtable of symbol {\tt s}. \\
1  \> : \> Symbol {\tt z} has index 1 in the symboltable of symbol {\tt s}.\\
   \> : \> Because there is only one term over symbol {\tt z}, no bits are\\
   \>   \> needed to encode this term. Now we only need to encode the \\
   \>   \> second argument of the input term, {\tt s(z)}. \\
   \> : \> No bits are needed to encode the function symbol {\tt s}, because\\
   \>   \> it is the only symbol that can occur as the second argument of mult.\\
1  \> : \> {\tt s(z)} has index 1 in the termtable of symbol {\tt s}. Because\\
   \>   \> this term has already been written, we are done.
\end{tabbing}

Only five bits were needed to encode the term {\tt mult(s(s(z)),s(z))}
after some information about the symbols was written! As mentioned
earlier, the amount of data needed to write the symbols is in most
cases negligible compared to the actual term data so the extra
information stored with the symbols really pays of in the long run.

\subsubsection{Compression results}
The compression ratio that can be reached using the techniques discussed
in this section depend on characteristics of the term being written,
like the amount of sharing, the number of different symbols, and the
distribution of terms over these symbols. In practice we reach a compression
ratio between two and twelve compared to the textual representation
of the term. Reading \baf\ terms is about a factor two to four faster
than reading their textual representation. Writing of \baf\ terms is
only a fraction slower than writing their textual representation.

%}}}
%{{{ Applications

\section{Applications}

The \ATerm\ library is developed because there was need for such a generic and
flexible datatype. The \ATerm\ library is already applied in quite a number
of projects. In this section we will briefly discuss a number of these
projects. Quite a number of these projects are triggered by the development
of the new \asmetaenv\ \cite{BKMO97}.

\subsection{Representing Parse Trees: AsFix, CasFix, etc.}

A very first version of a \ATerm\ library was developed in combination
with the \TB\ \cite{BK98}. This first version was used to represent
terms which were transported between tools connected to the \TB. 
Parallel to the development of the \TB, a formalism was developed
to represent parse trees \cite{??}.
The requirements for this formalism were:
\begin{itemize}
\item It should be easy machine processable.
\item It should have a textual representation for human inspection.
\item It should provide a mechanism to store auxilary information.
\item The parse trees should be self-contained, thus all applied
production rules must be available as well as the original layout
and comments.
\end{itemize}
The first version of this formalism to represent parse trees was quite
similar to the term representation used in the \TB.
This observation triggered the combination of both datatypes into the
\ATerm\ datatype.

The \ATerm\ datatype proved to be a powerful and flexible mechanism
to represent (abstract) syntax trees. By defining an appropriate set
of function names specific formalisms for representing parse trees
and (abstract) syntax trees can be created.

\subsubsection{AsFix}

\asfix\ is a formalism to represent \asdf\ modules and terms. Instead
of giving the abstract syntax representation of modules and terms, it
was decided to give the parse tree representation which not only
contains the applied syntax rules but also all original layout and
comments. This makes it possible to develop tools, such as (structure)
editors and rewrite engines, which process \asfix\
terms without consulting a common database.

\ATerms\ can be used for representing parse trees
by instantiating function symbols with appropriate values.
Suppose we have the following function symbols:
\begin{itemize}
\item $\mbox{\tt prod}(T)$ represents production
rule $T$.
\item $\mbox{\tt appl}(T_1,T_2)$ represents
applying production rule $T_1$ to the arguments $T_2$.
\item $\mbox{\tt l}(T)$ represents literal $T$.
\item $\mbox{\tt sort}(T)$ represents sort $T$.
\item  $\mbox{\tt lex}(T_1,T_2)$ represents
(lexical) token $T_1$ of sort  $T_2$.
\item $\mbox{\tt w}(T)$ represents white space $T$.
\end{itemize}
With the functions defined above we can represent simple parse trees in
\aterms.

The following context-free syntax rules (in \sdf\ \cite{HHKR92.new}) are
necessary to parse the input sentence {\tt true or false}.

\begin{small}
\begin{verbatim}
sort Bool
context-free syntax
  true         -> Bool
  false        -> Bool
  Bool or Bool -> Bool {left}
\end{verbatim}
\end{small}
The parse tree below represents the input sentence {\tt true or false}
in \ATerms.

\begin{small}
\begin{verbatim}
appl(prod([sort("Bool"),l("or"),sort("Bool")],sort("Bool"),
          attrs([atr("left")])),
     [appl(prod([l("true")],sort("Bool"),no-attrs),[l("true")]),
      w(" "),l("or"),w(" "),
      appl(prod([l("false")],sort("Bool"),no-attrs),[l("false")])
     ])
\end{verbatim}
\end{small}
Note that this parse tree is completely self-contained and does not depend
on a separate grammar definition.
It is clear that this way of representing parse trees contains a lot of
redundant information. Therefor, both the maximal sharing and BAF are 
essential to reduce the size of these terms.

The annotations provided by \ATerm\ data type can be used to store
auxilary information like position information derived by the parser
or font and/or color information needed by the (structure) editors.
This information is globally available but can be ignored by tools
that are not interested in it.

\subsubsection{CasFix}

Casl is a new algebraic specification formalism \cite{casl1}
developed within the CoFI group. The goal of this CoFI group is to
develop a general algebraic specification formalism together with
a set of tools to support the development of Casl specifications.
The goal is to reuse existing tools as much as possible. In order
to let the various tools, e.g.\ parsers, editors, rewriters, proof checkers,
communicate with each other an intermediate format was needed.
It was decided that Casl specifications should not be exchanged on a
textual level but as abstract syntax trees. \ATerms\ are used to
represent the abstract syntax trees of Casl, this resulted in CasFix
\cite{BKO98.casl}.
CasFix is obtained by defining an appropriate set of function symbol
used in the abstract syntax representation and by defining a mapping
form the concrete syntax to the abstract syntax.
The abstract syntax of Casl is defined in \cite{casl1}, for each
abstract syntax rule an equivalent CasFix construct is defined:

Consider

\begin{small}
\begin{verbatim}
ALTERNATIVE ::= "total-construct" OP-NAME COMPONENTS* 
\end{verbatim}
\end{small}
$\Longrightarrow$
\begin{small}
\begin{verbatim}
total-construct(<OP-NAME>,COMPONENTS*([<COMPONENTS>]))
\end{verbatim}
\end{small}
in this example {\tt "total-construct"} and {\tt "COMPONENTS*"} are
function symbols and {\tt <OP-NAME>} and {\tt <COMPONENTS>} represent
the subtrees of the corresponding sort.

\subsection{ASF+SDF Meta-Environment}

The \asmetaenv\ \cite{Kli93.meta} is an interactive development environment
for writing language specifications in \asdf. 
A new generation of the
environment is being developed based on separate components connected
via the \TB\ \cite{BK98}, 
a description of the new architecture can be found in \cite{BKMO97}.
This new \metaenv\ provides tools for parsing,
compilation, rewriting, debugging, and formatting.
With respect to parsing there is a two level approach, first the
specification is being parsed and based on the syntax defined in 
this specification a parse table is generated in
order to be able to parse terms over this syntax, this can be considered
as a second level of parsing.

\asfix plays an important role in this 
new \metaenv. The \asdf\ modules of the specification after parsing
are stored as \asfix\ terms. Information concerning the specification
such as the rewrite rules that must be compiled or interpreted are
exchanged as \asfix\ terms.

The \aterm\ format is directly used by the parser generator and the
parse table interpreter \cite{Vis97} because the parse table is
represented as an \aterm. \aterms\ are also used with in the \asdf2C
compiler, see the next section.

\subsection{ASF+SDF to C compiler}

\subsection{JFG}

%}}}
%{{{ Related Work

\section{Related Work}

\begin{itemize}
\item IDL
\item SGML
\item GEL
\end{itemize}

%}}}
%{{{ Conclusions

\section{Conclusions}

\begin{itemize}
\item The average garbage collection time for the applications we developed
using the \ATerm\ library is less then 10 percent.
\item The \ATerm\ library is powerful enough to develop both memory
and computational applications, e.g., the {\sc Asf+Sdf} compiler \cite{BOHK98}
is entirely based on this library and the application developed by
JanFriso Groote.
\item The current implementation is running on SGI, Solaris, and Linux.
\item The \ATerm\ library gave rise to an unexpected application,
a tool for detecting code clones in COBOL programs.
\item The using BAF reduces the term size by a factor  and reduces the
processing time by a factor XXX.
\end{itemize}

%}}}
%{{{ Future Work

\section{Future Work}

\begin{itemize}
\item Generational garbage collector.
\item BAF for JAVA.
\item More compact BAF format.
\item Porting to Windows platform.
\end{itemize}

%}}}
