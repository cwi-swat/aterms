%{{{ Introduction

\section{Introduction} \label{intro}
Cut and paste operations on complex data structures are standard in
most desktop software environments: one can easily clip a part of a
spreadsheet and paste it into  a text document.  The exchange of complex
data is also common in distributed applications: complex queries,
transaction records, and more complex data are exchanged between
different parts of a distributed application.  Compilers and
programming environments consist of tools such as editors, parsers,
optimizers, and code generators that exchange syntax trees,
intermediate code, and the like.

\begin{sloppypar}
How is this exchange of complex data structures between applications
achieved?  One solution is Microsoft's Object Linking and Embedding
(OLE)~\cite{OLE}.  This is a platform-specific, proprietary, set of
primitives to construct Windows applications. Another,
language-specific, solution is to use Java's serialization
interface~\cite{Java}. This allows writing and reading Java objects as
sequential byte streams. Yet another solution is to use OMG's
Interface Definition Language (part of the Common Object Broker
Architecture~\cite{OMG-IDL}) to define data structures in a
language-neutral way. Specific language-bindings provide the mapping
from IDL data structures to language-specific data structures.
\end{sloppypar}

All these solutions have their merits but do not really qualify when
looking for an \emph{open}, \emph{simple}, \emph{efficient},
\emph{concise}, and \emph{language independent} solution for the
exchange of complex data structures between distributed applications.
To be more specific, we are interested in a solution with the following
characteristics:

\begin{description}

\item{\emph{Open\/}:} independent of any specific hardware or software platform.

\item{\emph{Simple\/}:} the procedural interface should contain 10 rather
than 100 functions.

\item{\emph{Efficient\/}:} operations on data structures should be fast.

\item{\emph{Concise\/}:} inside an application the storage of data structures
should be as small as possible by using compact representations and by
exploiting sharing. Between applications the transmission of data
structures should be fast by using a compressed representation with
fast encoding and decoding.  Transmission should preserve any
sharing of in-memory representation in the data structures.

\item{\emph{Language-independent\/}:} data structures can be created and
manipulated in any suitable programming language.

\item{\emph{Annotations\/}:} applications can transparently extend
the main data structures with annotations of their own
to represent non-structural information.
\end{description}

In this paper we describe the data type of \emph{Annotated Terms}, or
just \emph{ATerms}, that have the above characteristics.  They form a
solution for our implementation needs in the areas of interactive
programming environments \cite{Kli93.meta,BKMO97} and distributed
applications~\cite{BK98} but are more widely applicable.
Typically, we want to exchange and process tree-like data structures
such as parse trees, abstract syntax trees, parse tables, generated
code, and formatted source texts. The applications involved include
parsers, type checkers, compilers, formatters, syntax-directed editors,
and user-interfaces written in a variety of languages.  Typically, a
parser may add annotations to nodes in the tree describing the
coordinates of their corresponding source text and a formatter may add
font or color information to be used by an editor when displaying the
textual representation of the tree.

The \ATerm\ data type has been designed to represent such
tree-like data structures and it is therefore very natural to use
\ATerms\ both for the internal representation of data inside an
application and for the exchange of information between applications.
Besides function applications that are needed to represent the basic
tree structure, a small number of other primitives are provided to
make the \ATerm\ data type more generally applicable.  These include
integer constants, real number constants, binary large data objects
(``blobs''), lists of \ATerms, and placeholders
to represent typed gaps in \ATerms.  Using the
comprehensive set of primitives and operations on \ATerms, it is
possible to perform operations on an \ATerm\ received from another
application without first converting it to an application-specific
representation.

First, we will give a quick overview of \ATerms\
(Section~\ref{aterms-at-a-glance}).  Next, we discuss implementation issues
(Section~\ref{implementation}) and give some insight in performance issues
(Section~\ref{measurements}).
An overview of applications (Section~\ref{applications}) and an
overview of related work and a 
discussion (Section~\ref{discussion}) conclude this paper.

%}}}
%{{{ ATerms at a Glance

\section{ATerms at a Glance} \label{aterms-at-a-glance}

We now describe the constructors of the
\ATerm\ data type (Section~\ref{aterm-datatype}) and
the operations defined on it (Section~\ref{aterm-operations}).

%}}}
%{{{ The ATerm datatype

\subsection{The \ATerm\ Data Type}
\label{aterm-datatype}

The data type of \ATerms\ ({\tt ATerm}) is defined as follows:

\begin{itemize}
\item  {\tt INT}: An integer constant (32-bits integer) is an \ATerm.\footnote{
We are currently upgrading the \aterm\ library
to support 64-bit architectures as well.}

\item {\tt REAL}: A real constant (64-bits real) is an \ATerm.

\item {\tt APPL}: A function application consisting of a function symbol and
      zero or more \ATerms\ (arguments) is an \ATerm.
      The number of arguments of the function is called
      the \emph{arity} of the function.

\item {\tt LIST}: A list of zero or more \ATerms\ is an \ATerm.

\item {\tt PLACEHOLDER}: A placeholder term containing an \ATerm\ representing the
      type of the placeholder is an \ATerm.

\item {\tt BLOB}: A ``blob'' (Binary Large data OBject) containing a length indication
      and a byte array of arbitrary (possibly very large) binary data 
      is an \ATerm.

\item A list of \ATerm\ pairs may be associated with every \ATerm\
      representing a list of $(label,annotation)$ pairs.

\end{itemize}

Each of these constructs except the last one (i.e., {\tt INT}, {\tt
REAL}, {\tt APPL}, {\tt LIST}, {\tt PLACEHOLDER}, and {\tt BLOB}) form
subtypes of the data type {\tt ATerm}.
These subtypes are needed when determining the type of an arbitrary
\ATerm.  Depending on the actual implementation language they will be
represented as a constant (C, Pascal) or a subclass (C++, Java).

The last construct is the \emph{annotation construct}, which makes it
possible to annotate terms with transparent
information\footnote{Transparent in the sense that the result of most
operations is independent of the annotations. This makes it easy to
completely ignore annotations. Examples of the use of
annotations include annotating parse trees with positional or
typesetting information, and annotating abstract syntax trees with the
results of type checking.}.

Appendix \ref{concrete-syntax} contains a definition of the concrete
syntax of \ATerms.  The primary reason for having a concrete syntax is
to be able to exchange \ATerms\ in a human-readable
form. In Section \ref{implementation} we also discuss a compact binary
format for the exchange of \ATerms\ in a format that is only suitable
for processing by machine.  We will now give a number of examples to
show some of the features of the textual representation of \ATerms.

\begin{itemize}
\item Integer and real constants are written conventionally:
      {\tt 1}, {\tt 3.14}, and {\tt -0.7E34} are all valid \ATerms.

\item Function applications are represented by a function name followed
      by an open parenthesis, a list of arguments separated
      by commas, and a closing parenthesis. When there are no
      arguments, the parentheses may be omitted. Examples are:
      {\tt f(a,b)} and {\tt "test!"(1,2.1,"Hello world!")}.
      These examples show that double quotes can be used to delimit
      function names that are not identifiers.

\item Lists are represented by an opening square bracket, a number of
      list elements separated by commas and a closing square bracket:
      {\tt [1,2,"abc"]}, {\tt []}, and {\tt [f,g([1,2]),x]} are
      examples.

\item A placeholder is represented by an opening angular bracket
      followed by a subterm and a closing angular bracket. Examples are
      {\tt <int>}, {\tt <[3]>}, and {\tt <f(<int>,<real>)>}.

\item Blobs do not have a concrete syntax because their human-readable form
      depends on the actual blob content.
\end{itemize}

\subsection{Operations on ATerms}
\label{aterm-operations}

The operations on \ATerms\ fall into three categories: making and
matching \ATerms\ (Section~\ref{making}), 
reading and writing  \ATerms\ (Section~\ref{reading}),
and annotating \ATerms\ (Section~\ref{annotating}). The total
of only 13 functions provide enough functionality for most users to
build simple applications with \ATerms.  We refer to this interface as
the \emph{level one} interface of the \ATerm\ data type.

To accommodate ``power'' users of \ATerms\ we also provide a
\emph{level two} interface, which contains a more sophisticated set of
data types and functions. It is typically used in generated C code
that calls \ATerm\ primitives, or in efficiency-critical applications.
These extensions are useful only when more control over the underlying
implementation is needed or in situations where some operations that
can be implemented using level one constructs can be expressed more
concisely and implemented more efficiently using level two constructs.
The level two interface is a strict superset of the level one
interface (see Appendix \ref{level2} for further details).

Observe that \ATerms\ are a purely functional data type and that no
destructive updates are possible, see Section \ref{max-sharing} 
for more details.

\subsubsection{Making and Matching \ATerms} \label{making}

The simplicity of the level one interface is achieved by the
\emph{make-and-match} paradigm:

\begin{itemize}
\item \emph{make}  (compose) a new \ATerm\ by providing a pattern for it and filling in the holes in the pattern.
\item \emph{match} (decompose) an existing \ATerm\ by comparing it with a pattern
and decompose it according to this pattern.
\end{itemize}

Patterns are just \ATerms\ containing placeholders.  These
placeholders determine the places where \ATerms\ must be substituted
or matched.  An example of a pattern is {\tt "and(<int>,<appl>)"}.
These patterns appear as string argument of both make and match and
are remotely comparable to the format strings in the {\tt printf}/{\tt
scanf} functions in C.
The operations for making and matching \ATerms\ are:

\begin{itemize}
\begin{sloppypar}
\item {\tt ATerm ATmake(String $p$, ATerm $a_1$, ..., ATerm $a_n$)}:
         Create a new
term by taking the string pattern $p$, parsing it as an \ATerm\ and filling
the placeholders in the resulting term with values taken from $a_1$
through $a_n$.
If the parse fails, a message is printed and the program is aborted.
The types of the arguments depend on the specific placeholders used
in \emph{pattern}. For instance, when the placeholder {\tt <int>} is used
an integer is expected as argument and a new integer ATerm is constructed.
\end{sloppypar}

\item 
\begin{sloppypar}
{\tt ATbool ATmatch(ATerm $t$, String $p$, ATerm *$a_1$, ..., ATerm *$a_n$)}:

Match term $t$ against
pattern $p$, and bind subterms that match with placeholders in
$p$ with the result variables $a_1$ through $a_n$.
Again, the type of the result variables depend on the placeholders
used.
If the parse of pattern $p$ fails, a message is printed and the program 
is aborted.
If the term itself contains placeholders these may occur in the 
resulting substitutions.
The function returns {\tt true} when the match succeeds,
{\tt false} otherwise.
\end{sloppypar}

\item {\tt Boolean ATisEqual(ATerm $t_1$, ATerm $t_2$)}: 
Check whether two \ATerms\ are 
equal. The annotations of $t_1$ and $t_2$ must be equal as well.

\item {\tt Integer ATgetType(ATerm $t$)}: Retrieves the type of an
\ATerm. This operation returns one of the subtypes mentioned before in
Section~\ref{aterm-datatype}.

\end{itemize}

\subsubsection{Reading and Writing \ATerms} \label{reading}

For reasons of efficiency and conciseness, reading and writing can
take place in two forms: text and binary.  
The text format uses the textual representation discussed
earlier in Section~\ref{aterm-datatype} and Appendix
\ref{concrete-syntax}.  This format is human-readable,
space-inefficient\footnote{ The unnecessary size explosion could be
avoided by extending the textual representation with a mechanism for
labeling and referring to terms.  Instead of \texttt{f(g(a),g(a))},
one could then write \texttt{f(1:g(a), \#1)}. The first occurrence of
\texttt{g(a)} is labeled with ``\texttt{1}'', and the second
occurrence refers to this label (``\texttt{\#1}'').},
and any sharing of the in-memory representation of terms is lost.

The binary format (Binary ATerm Format, see Section~\ref{baf}) is 
portable, machine-readable, very compact, 
and preserves all in-memory sharing.
The operations for reading and writing \ATerms\ are:

\begin{itemize}

\item {\tt ATerm ATreadFromString(String $s$)}: Creates a new term 
by parsing
the string $s$. When a parse error occurs, a message is printed, and
a special error value is returned.

\item {\tt ATerm ATreadFromTextFile(File $f$)}: Creates a new term by 
parsing the data from file $f$. Again, parse errors result in a 
message being printed and an error value being returned.

\item {\tt ATerm ATreadFromBinaryFile(File $f$)}: Creates a new term by reading a
binary representation from file $f$. 

\item {\tt Boolean ATwriteToTextFile(ATerm $t$, File $f$)}: Write the text
representation of term $t$ to file $f$. Returns {\tt true} for
success and {\tt false} for failure.

\item {\tt Boolean ATwriteToBinaryFile(ATerm $t$, File $f$)}: Write a 
binary representation of term $t$ to file $f$. Returns
{\tt true} for success, and {\tt false} for failure.

\item {\tt String ATwriteToString(ATerm $t$)}: Return the text representation
of term $t$ as a string.

\end{itemize}

Either format (textual or binary) can be used on any linear stream, including files, sockets,
pipes, etc.

\subsubsection{Annotating \ATerms} \label{annotating}

Annotations are $(label,annotation)$ pairs that may be attached to an
\ATerm. Recall that \ATerms\ are a completely functional data type and
that no destructive updates are possible. This is evident in the
following operations for manipulating annotations:

\begin{itemize}

\item 
\begin{sloppypar}
{\tt ATerm ATsetAnnotation(ATerm $t$, ATerm $l$, ATerm $a$)}:
Return a copy of term $t$ in which the annotation labeled with $l$ has been changed
into $a$. If $t$  does not have an annotation with
the specified label, it is added.
\end{sloppypar}

\item {\tt ATerm ATgetAnnotation(ATerm $t$, ATerm $l$)}:
Retrieve the annotation labeled with $l$ from term $t$.
If $t$ does not have an annotation with the specified label,
a special error value is returned.

\item {\tt ATerm ATremoveAnnotation(ATerm $t$, ATerm $l$)}: Return a copy
of term $t$ from which the annotation labeled with $l$ has been removed.
If $t$ does not have an annotation with the specified label,
it is returned unchanged.

\end{itemize}

 

%}}}

\section{Implementation} \label{implementation}

%{{{ Requirements

\subsection{Requirements}
\label{requirements}

In Section~\ref{intro} we have already mentioned our main
requirements: openness, simplicity, efficiency, conciseness,
language-independence, and capable of dealing with annotations.  
There are a number of other
issues to consider that have a great impact on the implementation, and
that make this a fairly unique problem:

\begin{itemize}

\item By providing automatic garbage collection \ATerm\ users
      do not need to deallocate \ATerm\ objects explicitly.
      This is safe and simple (for the user).

\item The expected lifetime of terms in most applications is very short.
      This means that garbage collection must be fast and should touch
      a minimal amount of memory locations to improve caching and paging 
      performance.

\item The total memory requirements of an application cannot be estimated
      in advance. It must be possible to allocate more memory incrementally.

\item Most applications exhibit a high level of redundancy in the terms
      being processed. Large terms often have a significant number of
      identical subterms. Intuitively this can be explained from the
      fact that most applications process terms with a fixed signature and
      a limited tree depth. When the amount of terms that is being processed
      increases, it is plausible that the similarity between terms also
      increases.

\item In typical applications less than 0.1 percent of all terms have
      an arity higher than 5.

\item Many applications will use annotations only sparingly. 
      The implementation should not impose a penalty on applications
      that do not use them.

\item In order to have a portable yet efficient implementation, the
      implementation language will be C. This poses some special 
      requirements on the garbage collection strategy\footnote{We have
      implemented the library in Java as well. 
      In this case, many of the issues
      we discuss in this paper are irrelevant, either because we can use 
      built-in features of Java (garbage collection), or
      because we just cannot express these low level concerns in Java.}.
\end{itemize}

With these considerations in mind, we will now discuss
maximal (in-memory) sharing of terms (Section~\ref{max-sharing}),
garbage collection (Section~\ref{garbage-collection}),
the encoding of terms (Section~\ref{term-encoding}), and
the Binary ATerm Format (Section~\ref{baf}).
     
%}}}
%{{{ Design

\subsection{Maximal Sharing}
\label{max-sharing}

Our strategy to minimize memory usage is simple but effective: we only
create terms that are \emph{new}, i.e., that do not exist already.  If
a term to be constructed already exists, that term is reused,
ensuring maximal sharing.  This strategy fully exploits the redundancy
that is typically present in the terms to be built and leads to
maximal sharing of subterms. The library functions that construct terms
make sure that shared terms are returned whenever possible.  The sharing of
terms is thus invisible to the library user.

\subsubsection{The Effects of Maximal Sharing}

Maximal sharing of terms can only be maintained when we check at every
term creation whether a particular term already exists or not. This
check implies a search through all existing terms but must
be fast in order not to impose an
unacceptable penalty on term creation.  Using a hash function that
depends on the internal code of the function symbol and the addresses
of its arguments, we can quickly search for a function application
before creating it.  The terms are stored in a hash table.  The hash
table does not contain the terms themselves, but pointers to the
terms. This provides a flexible mechanism of resizing the table and
ensures that all entries in the table are of equal size. Hence the
(modest but not negligible) cost at term creation time is one
hash table lookup.

Fortunately, we get two returns on this investment.  First, the
considerably reduced memory usage also leads to reduced
execution time.  Second, we gain substantially as the equality
check on terms ({\tt ATisEqual}) becomes very cheap: it reduces from
an operation that is linear in the number of subterms to be compared
to a constant operation (pointer equality).

Another consequence of our approach is less fortunate.  Because terms
can be shared without their creator knowing it, terms cannot be modified
without creating unwanted side-effects.  This means that terms
effectively become \emph{immutable} after creation.  Destructive
updates on maximally shared terms are not allowed.  Especially in list
operations, the fact that \ATerms\ are immutable can be expensive. It
is often the responsibility of the user of the library to choose
algorithms that minimize the effect of this shortcoming.

\subsubsection{Searching for Shared Subterms}

Maximal sharing of terms requires checking at term creation time whether
this term already exists. This search must be fast in order to ensure 
efficient term creation. A hash function based on the
\emph{addresses} of the function symbol and the arguments of a function
application allows for a quick lookup in the hash table to find a function 
application before creating it.

\paragraph{Collisions}
One issue in hash techniques is handling collisions. The simplest
technique is linear chaining~\cite{Knuth73}.
This requires one pointer in each
object for hash chaining, which in our implementation implies a memory
overhead of about 25 percent. Other solutions for collision
resolution will either increase the memory requirements, 
or the time needed for insertions or deletions (see \cite{Knuth73}).
We therefore use linear hash chaining in our implementation. 

\paragraph{Direct or Indirect Hashing}
Another issue is whether to store all terms directly in
the hash table, or only references.
Storing the objects directly in the hash table saves a
memory access when retrieving a term as well as the
space needed to store the reference.
However, there are severe drawbacks to this approach:
\begin{itemize}
\item We cannot rehash old terms because rehashing
      means that we have to move the objects in memory.
      When using C as an implementation language, moving
      objects in memory is not allowed because we can only determine
      a conservative root set and therefore are not allowed to change
      the pointers to roots.
      This would mean that the hash table could not grow beyond its 
      initial size.
\item Internal fragmentation is increased, because empty slots
      in the hash table are as large as the object instead of
      only one machine word.
\item We would need a separate hash table for each term size to
      decrease the internal fragmentation. 
\end{itemize}

Because of these problems, we use linear hash chaining
combined with indirect hashing.
When the load of the hash table reaches a certain threshold, 
we rehash into a larger table.

The user can increase the initial size of
the hash table to save on resizing and rehashing
operations. The \ATerm\ library provides facilities for defining hash tables
as well. This allows the implementation of a fast lookup mechanism
for \ATerms. User-defined hash tables are used, for instance, 
to implement memo-functions in the \asdf\ to C compiler (see
Section~\ref{asdf2c}).

%{{{ Garbage collection

\subsection{Garbage Collection} \label{garbage-collection}

\subsubsection{Which Technique?}
The most common strategies for automatic recycling of unused space
are reference counting, mark-compact collection, and 
mark-sweep collection. 
In our case, reference counting is not a valid alternative, because
it takes too much time and space and is very hard to implement 
in C.
Mark compact garbage collection is also unattractive because
it assumes that objects can be relocated. This is not the case
in C where we cannot identify \emph{all} references to an object.
We can only determine the root set conservatively which is good enough
for mark-sweep collection discussed below, but not for mark-compact collection.

\paragraph{Mark-sweep Garbage Collection}

Mark-sweep garbage collection works using three phases.
In the first phase, all objects on the heap are marked as `dead'.
In the second phase, all objects reachable from the known set of root
objects are marked as `live'. In the third phase, all `dead' objects
are swept into a list of free objects.

Mark-sweep garbage collection can be implemented in C efficiently,
and without support from the programmer or compiler \cite{BW88,Bo93}. 
Mark-sweep collection is more efficient,
both in time and space than reference counting \cite{JL96}. A possible
drawback is increased memory fragmentation compared to mark-compact collection.
The typical space overhead for a mark-sweep garbage collection algorithm is
only 1 bit per object, whereas a reference count field would take at
least three or four bytes.

\subsubsection{Reusing an Existing Garbage Collector}
A number of excellent generic garbage collectors for C are 
freely available, so why do we not reuse an existing implementation?

We have examined a number of alternatives, but none of them fit 
our needs. The Boehm-Weiser garbage collector \cite{BW88} came close,
but we face a number of unusual circumstances that render existing
garbage collectors impractical:

\begin{itemize}
\item The hash table always contains references to 
      all objects. It must be possible to instruct the garbage collector
      not to scan this area for roots.
\item After an object becomes garbage, it must also be removed from the
      hash table. This means that we need very low level control
      over the garbage collector.
\item The \ATerm\ data type has some special characteristics that can
      be exploited to dramatically increase performance:
      \begin{itemize}
        \item Destructive updates are not allowed. In garbage collection
              terminology, this means that there are no pointers from
              old objects to younger objects. Although we do not exploit
              it in the current implementation, this characteristic
              makes the use of a \emph{generational} garbage collector
              very attractive.
        \item The majority of objects have an in-memory 
	      representation of 8, 12, or 16 bytes.
        \item Practical experience has shown that not many root pointers
              are kept in static variables or on the generic C heap. 
              Performance can be increased
              dramatically if we eliminate the expensive scan through the
              heap and the static data area for root pointers. 
              The only downside is that we require
              the programmer to explicitly supply the set of roots
              that is located on the heap or in static variables.
      \end{itemize}
\end{itemize}
These observations allow us to gain efficiency on several
levels, using everything from low level system `hacks' to high-level 
optimizations.



%}}}

\subsubsection{Implementing the Garbage Collector}

Considering both performance and the maintainability of the code that
uses the \ATerm\ library, we have opted for a version of the
mark-sweep garbage collector.  Every object contains a single bit used
by the mark-sweep algorithm to indicate `live' (marked) objects. At
the start of a garbage collection cycle, all objects are unmarked. The
garbage collector tries to locate and mark all live objects by
traversing all terms that are explicitly protected by the programmer
(using the {\tt ATprotect} function), and by scanning the C run-time stack
looking for words that could be references to objects. When such a
word is found, the object (and the transitive closure of all of the
objects it refers to) are marked as `live'.

This scan of the run-time stack causes all objects referenced from local
variables to be protected from being garbage collected.
Our garbage collector 
is a  conservative collector in the sense that some of the words on the
stack could accidentally have the same bit pattern as object references.
Because there is no way to separate these `fake' bit patterns from
`real' object references, this can cause
objects to be marked as `live' when these are actually garbage.
Note that bit patterns on the stack that do not point to valid objects
are not traversed at all. Only when a bit pattern represents an address
that is a valid object address it is followed to mark the corresponding
object.

When all live objects are marked, a single sweep through the heap
is used to store all objects that are free in separate lists of free
objects, one list for each object size.

As we shall see in Section \ref{term-encoding}, most objects consist
of only a couple of machine words.  By restricting the maximum arity
of a function, we can also set an upper bound on the maximum size of
objects.  This enables us to base the memory management algorithms we
use on a small number of block sizes.  Allocation of objects is now
simply a matter of taking the first element from the appropriate
free-list, which is an extremely cheap operation.  If garbage
collection does not yield enough free objects, new memory blocks will
be allocated to satisfy allocation requests.

%}}}

%{{{ Term encoding

\subsection{Term Encoding} \label{term-encoding}

An important issue in the implementation of \ATerms\ is how to
represent this data type so that all operations can be performed
efficiently in time and space.

The very concise encoding of \ATerms\ we use is as follows.
Assume that one machine word consists of four bytes.
Every \ATerm\ object is stored in two or more machine words.
The first byte of the first word is called
the \emph{header} of the object, and consists of four fields (see
Figure \ref{header}):

\begin{figure}[tb]
  \centerline{\epsfig{file=header.eps,scale=0.6}}
  \caption{\label{header}The header layout}
\end{figure}
 
\begin{itemize}
\item A field consisting of one bit used as a mark flag by the garbage
      collector.
\item A field consisting of one bit indicating whether or not this term
      has an annotation.
\item A field consisting of three bits that indicate the type of the term.
\item A field consisting of three bits representing the arity (number of
      pointers to other terms) of this object. When this field
      contains the maximum value of 7, the term must be a function application
      and the actual arity can be found by retrieving the arity of the
      function symbol (see below).
\end{itemize}

Depending on the type of the node (as determined by the header byte in
the first word) the remaining bytes in the first word contain either a
function symbol, a length indication, or they are unused.

The \emph{second} word is always used for hashing, and links together
all terms in the same hash bucket.

The type of the node determines its exact layout and contents.  Figure
\ref{encoding} shows the encoding of the different term types which we
will now describe in more detail.

\begin{figure}[!htb]
  \centerline{\epsfig{file=encoding.eps,scale=0.6}}
  \caption{\label{encoding}Encoding of the different term types}
\end{figure}

\paragraph{INT encoding}
In an integer term, the third word contains the integer value.
The arity of an integer term is 0.

\paragraph{REAL encoding}
In an real term, the third and fourth word contain the real value 
represented by an 8 byte IEEE floating point number.
The arity of a real term is 0.

\paragraph{APPL encoding}
The remaining 3 bytes following the header in the first word are used to
represent the index in a table containing the function symbols. 
The words following the second word contain
references to the function arguments.
In this way, function applications can be encoded in
$2+n$ machine words, with $n$ the arity of the function application.

\paragraph{LIST encoding}
The binary list constructor can be seen as a special function
application with no function symbol and an arity of 2. The
third word points to the first element in the list, this is called
the {\tt first} field, the fourth word
points to the remainder of the list, and is called the {\tt next} field. 
The length of the list is
stored in the three bytes after the header in the first word.
The empty list\footnote{Due to the uniqueness of terms, only
one instance of the empty list is present at any time.}
is represented using a LIST object with empty
first and next fields, and a length of 0.

After the function application, the list construct is the second most 
used \ATerm\ construct. A (memory) efficient representation of lists is 
therefore very important. 
Due to the nature of the operations on \ATerm\ lists, there are
two obvious list representations: an array of term references or a linked
list of term references. Experiments have shown that in typical applications
quite varying list sizes are encountered. This renders the array approach 
inferior, because adding and deleting elements of a list would become too
expensive. Consequently, we have opted for the linked list approach.
Lists are constructed using binary list constructors, containing 
a reference to the first element in the list and to the tail of the list.
Each list operation must ensure that the list is ``normalized'' again.
This makes it very easy to perform the most commonly used operations
on list, namely adding or removing the first element of a list.

Other operations are more expensive, since we do not
allow destructive updates. Adding an element to the tail of a list
for instance, requires $n$ list creation operations, where $n$ is
the number of elements in the newly created list.

\paragraph{PLACEHOLDER encoding}
The placeholder term has an arity of 1, where the third
word contains a pointer to the placeholder type.

\paragraph{BLOB encoding}
The length of the data contained in a BLOB term is stored in
the three bytes after the header. 
This means that up to 16,777,200 bytes can be encoded in a single
BLOB term.
A pointer to the actual data is stored in the third word.

\paragraph{Annotations}
In all cases, annotations are represented using an extra word
at the end of the term object. The single annotation bit in the header
indicates whether or not an annotation is present. Only when this
bit is set, an extra word is allocated that points to a term
with type {\tt LIST}, which represents the list of annotations.

%}}}

%{{{ The Binary ATerm Format (BAF)

\subsection{\ATerm\ Exchange: the Binary \ATerm\ Format} \label{baf}

The efficient exchange of \ATerms\ 
between tools is very important. The simplest form of exchange 
is based on the concrete syntax presented in Appendix \ref{concrete-syntax}.
This would involve printing the term on one side and parsing it
on the other. The concrete syntax is not a very efficient exchange format
however, because the sharing of function symbols and subterms cannot 
be expressed in this way.

A better solution would be to exchange a representation in which
sharing (both of function symbols and subterms) can be expressed
concisely.  A raw memory dump cannot be used, because addresses in the
address space of one process have no meaning in the address space of
another process.

In order to address these problems, we have developed \baf, the
{\bf B}inary {\bf A}Term {\bf F}ormat.
Instead of writing addresses, we assign a unique number (index)
to each subterm and each symbol occurring in a term that we want to 
exchange. When referring to this term, we could use its index instead
of its address. 

When writing a term, we begin by writing a table (in order of
increasing indices) of all function symbols used in this term. Each
function symbol consists of the string representation of its name
followed by its arity.

{\em ATerms are written in prefix order.}
To write a function application, first the index of the function
symbol is written. Then the indices of the arguments are written. When
an argument consists of a term that has not been written yet, the
index of the argument is first written itself before continuing with
the next argument.  In this way, every subterm is written exactly
once. Every time a parent term wishes to refer to a subterm, it just
uses the subterm's index.

\subsubsection{Exploiting \ATerm\ Regularities}
When sending a large term containing many subterms, the subterm
indices can become quite large. Consequently many bits are needed
to represent these indices. We can considerably reduce the size of
these indices when we take into account some of the regularities in
the structure of terms. Empirical study shows that the set of function
symbols that can actually occur at each of the argument positions of a
function application with a given function symbol is often very small.
A explanation for this is that although \ATerm\ applications themselves
are not typed, the data types they represent often are. In this case,
function applications represent objects and the type of the object is
represented by the function symbol. The type hierarchy determines
which types can occur at each position in the object.

We exploit this knowledge by grouping all terms according to their top
function symbol. Terms that are not function applications are grouped
based on dummy function symbols, one for each term type.  For each
function symbol, we determine which function symbols can occur at each
argument position. When writing the table of function symbols at the
start of the BAF file, we write this information as well.  In most
cases this number of function symbol occurrences is very small
compared to the number of terms that is to be written. Storing some
extra information for every function symbol in order to get better
compression is therefore worthwhile.

When writing the argument of a function application, we start
by writing the actual symbol of the argument. Because this symbol
is taken from a limited set of function symbols (only those symbols
that can actually occur at this position), we can use a very small
number to represent it. Following this function symbol we
write the index of the argument term itself in the table of terms
over this function symbol instead of the index of the argument in
the total term table.

\subsubsection{Example}
As an example, we show how the term {\tt mult(s(s(z)),s(z))} is represented
in \baf. This term contains three function symbols: {\tt mult} with
arity two, {\tt s} with arity one, and {\tt z} with arity zero.
When grouping the subterms by function symbol we get:
\vspace{0.5cm}

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
0: {\tt mult}                   & 1: {\tt s}    & 2: {\tt z}       \\
\hline
{\tt mult(s(s(z)),s(z))}        & {\tt s(s(z))} & {\tt z} \\
                                & {\tt s(z)}    &   \\
\hline
\end{tabular}
\end{center}
\vspace{0.5cm}

\noindent When we look at the function symbols that can occur
at every argument position ($\geq 0$) we get:
\vspace{0.5cm}

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
position & {\tt mult}               & {\tt s}                   & {\tt z} \\
\hline
0        & {\tt s}                  & {\tt s}, {\tt z}  & \\
1        & {\tt s}                  &                           & \\
\hline
\end{tabular}
\end{center}
\vspace{0.5cm}

We start by writing this symbol information to file. To do this,
we have to write the following bytes\footnote{When the value of these numbers
used exceeds 127, two or more bytes are used to encode them. Strings are
written as strings to improve readability.}:

\begin{tabbing}
4 {\tt "mult"} \= : \= The length ({\tt 4}) and \ascii\ representation of {\tt mult}. \\
2              \> : \> The arity ({\tt 2}) of {\tt mult}. \\
1 1            \> : \> There is only one symbol ({\tt 1}) that can occur at the first argument\\
               \>   \> position of {\tt mult}. This is symbol {\tt s} with index ({\tt 1})\\
1 1            \> : \> At the second argument position, there is only ({\tt 1}) possible \\
               \>   \> top symbol and that is {\tt s} with index ({\tt 1}). \\
1 {\tt "s"}    \> : \> The length ({\tt 1}) and \ascii\ representation of {\tt s}.\\
1              \> : \> The arity ({\tt 1}) of {\tt s}.\\
2 1 2          \> : \> The single argument of {\tt s} can be either of two ({\tt 2}) different top \\
               \>   \> function symbols: {\tt s} with index ({\tt 1}) or {\tt z} with index ({\tt 2}). \\
1 {\tt "z"}    \> : \> The length ({\tt 1}) and \ascii\ representation of {\tt z}.\\
0              \> : \> The arity ({\tt 0}) of {\tt z}. \\
\end{tabbing}


\noindent Following this symbol information, the actual term {\tt
mult(s(s(z)),s(z))} can be encoded using only a handful of bits. Note
that the first function symbol in the symbol table is always the top
function symbol of the term (in this case: {\tt mult}):

\begin{tabbing}
00 \= : \= \kill
   \> : \> No bits need to be written to identify the function symbol {\tt s},\\
   \>   \> because it is the only possible function symbol at the first \\
   \>   \> argument position of {\tt mult}. \\
0  \> : \> One bit indicates which term over the function symbol {\tt s} is \\
   \>   \> written ({\tt s(s(z))}). Because this term has not been written yet,\\
   \>   \> it is done so now. \\
0  \> : \> The function symbol of the only argument of {\tt s(s(z))} is {\tt s}.\\
1  \> : \> {\tt s(z)} has index 1 in the term table of symbol {\tt s}. \\
1  \> : \> Symbol {\tt z} has index 1 in the symbol table of symbol {\tt s}.\\
   \> : \> Because there is only one term over symbol {\tt z}, no bits are\\
   \>   \> needed to encode this term. Now we only need to encode the \\
   \>   \> second argument of the input term, {\tt s(z)}. \\
   \> : \> No bits are needed to encode the function symbol {\tt s}, because\\
   \>   \> it is the only symbol that can occur as the second argument of mult.\\
1  \> : \> {\tt s(z)} has index 1 in the term table of symbol {\tt s}. Because\\
   \>   \> this term has already been written, we are done.
\end{tabbing}

Only five bits are thus needed to encode the term {\tt
mult(s(s(z)),s(z))}.  As mentioned earlier, the amount of data needed
to write the table of function symbols at the start of the \baf\ file
is in most cases negligible compared to the actual term data.

%}}}

%{{{ Measurements

\section{Performance Measurements} \label{measurements}

\subsection{Benchmarks}

How concise is the ATerm representation and how fast can \baf\ files
be read and written? Since results highly depend on the actual terms being used,
we will base our measurements on a collection of terms that cover most applications
we have encountered so far.

\subsubsection{Artificial Cases}
Two artificial cases are used that have been constructed to act
as borderline cases:

\begin{description}
\item[Random-unique:] a randomly generated term over a signature of 9 fixed function symbols
with arities ranging from 1 to 9 and an arbitrary number of constant
symbols (functions with arity 0). The terms are generated in such a
way that all constants are unique.  These terms are the worst case for
our implementation: there is no regularity to exploit and there are
many subterms with a relatively high arity.

\item[Random:] a randomly generated term over a signature of 10 function symbols
with arities ranging from 0 to 9. In these terms only a single
constant can occur which will be shared, but no other regularities can
be exploited and there are many subterms with a relatively high arity.

\end{description}

\subsubsection{Real Cases}

Several real-life cases are used that are based on actual applications:

\begin{description}
\item[COBOL Parse Table:] a generated parse table for COBOL including
embedded SQL and CICS.  The grammar consists of 2,009 productions and
the generated automaton has 6,699 states.  The parse table contains an
action-table (2,0947 non-empty entries) and a goto-table (76527
non-empty entries).  This is an example of an abstract data type
represented as \ATerm.

\item[COBOL System:] a COBOL system consisting of 117 programs with
a total of 247,548 lines of COBOL source code. It has been parsed with
the above parse table.  The parse trees constructed for these COBOL
programs are represented as \ATerms, see Section \ref{asfix} for more
details.

\item[Risla Library:] a parse tree of the component library for the Risla
language, a domain specific language for describing financial products
\cite{ADR95}.
This component library consists of 10,832 lines of code.

\item[LPO:] a linear process operator (LPO) describing the ``firewire''
protocol with 1 bus and 9 links \cite{GL99, Lut99}.
LPOs are the kernel of the $\mu$CRL ToolKit \cite{DG95}
which is a collection of tools for manipulation process and
data descriptions in $\mu$CRL (micro Common Representation 
Language) \cite{GP95}.
An LPO is a structured process, where the state consists of an assignment
to a sequence of typed data variables and its behaviour is described
by condition, action and effect functions.
These states are represented as \aterms, and are rather complex.

\item[Casl specifications:] a collection of abstract syntax trees 
represented as \aterms\ of
98 Casl files,
the total number of lines of Casl code is 2,506.
For more details on Casl and the abstract syntax tree representation
as \aterms\ we refer to Section \ref{casfix}. 

\item[lcc Parse Forest:] a new back-end similar to the ASDL back-end
\cite{Wan97} has been added to the lcc compiler \cite{Han99}.
This back-end maps the internal format used by
the lcc compiler to \aterms.
The \ATerm\ representation and the ASDL representation of a C program
contain equivalent information.

Given this back-end the C sources of the lcc compiler itself are mapped
to \aterms. 
The lcc compiler consists of 34 source files, consisting of a total of
13,588 lines of source code.

\item[S-expressions:] a simple translator has been developed which
transforms an S-expression into an \ATerm. This translator has been
used to process an arbitrary collection of ``.el'' files
containing S-expressions found within the Emacs source tree
under Linux.
The total number of ``.el'' files was 738, these files together contained 
286,973 lines of code.
\end{description}

\noindent In the cases of the COBOL System, Casl Specifications, lcc Parse
Forest, and S-Expressions the set of \aterms\ are combined into and
processed as {\em one} \aterm.
Measurements were performed on an ULTRA SPARC-5 (270 MHz) with 256 Mb
of memory. All times measured are the user CPU time for that particular job. 

\subsection{Measurements}

\begin{table}[tb]
\begin{center}
\begin{tabular}{|l||r|r|r|r|r|} \hline
Term                    & \multicolumn{1}{c|}{\# nodes}  
                        & \multicolumn{1}{c|}{\# unique} 
                        & \multicolumn{1}{c|}{Sharing}
                        & \multicolumn{1}{c|}{Memory}
                        & \multicolumn{1}{c|}{Bytes/}  \\
                        &                       
                        & \multicolumn{1}{c|}{nodes}
                        & \multicolumn{1}{c|}{(\%)}
                        & \multicolumn{1}{c|}{(bytes)}      
                        & \multicolumn{1}{c|}{Node}    \\ \hline \hline
\multicolumn{6}{|c|}{Artificial Cases}\\ \hline
Random-unique           &  1,000,000 &   1,000,000 &  0.00 & 15,198,694 & 15.20   \\ \hline 
Random                  &  1,000,000 &     92,246 &  90.81 &  2,997,120 &  3.00   \\ \hline \hline

\multicolumn{6}{|c|}{Real Cases}\\ \hline

COBOL Parse Table       &   961,070 &     97,516 &    89.85 & 2,836,529 &  2.95   \\ \hline
COBOL System            & 31,332,871 &    470,872 &   98.50 & 12,896,609 & 0.41   \\ \hline
Risla Library           &   708,838 &     40,073 &    94.35 &  960,170 &   1.35   \\ \hline
LPO                     &  8,894,391 &    225,229 &   97.47 &  3,701,438 & 0.42   \\ \hline
Casl Specifications     &    34,526 &     11,699 &   66.12 &   235,655 &   6.83   \\ \hline
lcc Parse Forest        &   360,829 &     86,589 &   76.00 &  1,547,713 &  4.29   \\ \hline
S-expressions		&   593,874 &    283,891 &   52.20 &  9,111,863 & 15.34  \\ \hline
\hline
Real Case Averages      &           &            &   82.07 &            &  4.51   \\ \hline
\end{tabular}
\caption{Memory usage of \ATerms} \label{measurements-in-memory}
\end{center}
\end{table}

In Table~\ref{measurements-in-memory}, we give results for the memory usage
of our sample terms\footnote{Since we consider the Random-unique and
Random cases to be unrepresentative, we only present the averages for the real
cases in this and the following tables.}. The five columns give the total
number of nodes in each term, the number of unique nodes in each term, the
sharing percentage, the amount of memory (in bytes) used for the storage of
the term, and the average number of bytes needed per node. As can be seen
in these figures, at least in our applications sharing \emph{does} make a
difference. By fully exploiting the redundancies in the input terms, we can
store a node using on the average 4.5 bytes, and still perform operations
on them efficiently.  The worst case behaviour is 15 bytes per node.
The amount of sharing is clearly less high in case of abstract syntax
trees than in case of parse trees represented as \asfix\ terms.
The \asfix\ terms contain much redundant information which can
be optimally shared.
The amount of sharing in the abstract syntax trees for Casl is
lower, but this is due to the fact that the set of
Casl specifications is small and each specification tests another
feature of the Casl language, so not much sharing was to be expected.
The S-expressions have the lowest ratio of sharing, but this was to
be expected: they represent ad hoc {\em hand-written}
Lisp programs while in the other cases the \ATerms\ are obtained by
a systematic translation from source code. In the latter case,
recurring patterns in the translation scheme result in higher levels of sharing.

\begin{figure}
\centerline{\epsfig{file=sharing.eps}}
\caption{\label{cobol-sharing}Sharing of a large number of COBOL parse trees}
\end{figure}
Figure \ref{cobol-sharing} shows the amount of sharing with respect to
the size of a large number of COBOL programs. Three different sets of
COBOL programs were considered. The first system consists of 151
files, the second of 116 files, and the last of 98 files. From this
figure it can be concluded that the amount of sharing increases with
the size of the COBOL system.  In all three systems, the percentage of
sharing converges to slightly over 90\%. We find this high percentage
in combination with the strong correlation between size and sharing very
remarkable and will analyze its causes and consequences in further detail
in a separate paper.

\begin{table}[tb]
\begin{center}
\begin{tabular}{|l||r|r|r|r|r|r|} \hline
Term                    & \multicolumn{1}{c|}{\ascii}
                        & \multicolumn{1}{c|}{Bytes/}
                        & \multicolumn{1}{c|}{Read}
                        & \multicolumn{1}{c|}{Read/}
                        & \multicolumn{1}{c|}{Write}
                        & \multicolumn{1}{c|}{Write/}\\

                        & 
                        & \multicolumn{1}{c|}{Node}
                        &       
                        & \multicolumn{1}{c|}{Node}
                        &       
                        & \multicolumn{1}{c|}{Node} \\

                        & \multicolumn{1}{c|}{(bytes)}   
                        &      
                        & \multicolumn{1}{c|}{(s)}
                        & \multicolumn{1}{c|}{($\mu$s)}
                        & \multicolumn{1}{c|}{(s)}
                        & \multicolumn{1}{c|}{($\mu$s)}\\ \hline \hline
\multicolumn{7}{|c|}{Artificial Cases}\\ \hline
Random-unique           &6,888,889 & 6.89 & 34.76 & 34.76 &  4.06 & 4.06  \\ \hline
Random                  &6,200,251 & 6.20 & 15.90 & 15.90 &  3.67 & 3.67  \\ \hline \hline

\multicolumn{7}{|c|}{Real Cases}\\ \hline
COBOL Parse Table   &  4,211,366 & 4.38 &  6.33 &  6.95 &  2.30 & 2.29  \\ \hline
COBOL System        &135,350,005 & 4.32 &199.43 &  6.36 & 65.02 & 2.08  \\ \hline
Risla Library       &  2,955,964 & 4.17 &  4.25 &  6.00 &  1.40 & 1.98  \\ \hline
LPO                 & 41,227,481 & 4.64 & 81.90 &  9.21 & 29.16 & 3.28  \\ \hline 
Casl Specifications &    217,958 & 6.31 &  0.36 & 10.43 &  0.08 & 2.32  \\ \hline
lcc Parse Fores     &  2,132,245 & 6.22 &  3.13 &  9.14 &  0.86 & 2.51  \\ \hline
S-expressions       &  7,954,550 &13.39 &15.09 & 25.41 &  2.49 & 4.19  \\ \hline
\hline
Real Case Averages      &        & 6.20 &       & 10.50 &       & 2.66 \\ \hline
\end{tabular}
\caption{Reading and writing \ATerms\ as \ascii\ text} \label{measurements-text}
\end{center}
\end{table}

In Table~\ref{measurements-text} we give results for reading and writing
our sample terms as \ascii\ text files. 
The six columns give the size of the text
representation of the test term in bytes, the average number of bytes per node,
the time needed to read the text file, the average time needed to read a node,
the time needed to write the text file, and the average time needed to write a node.
On the average, a node requires 6.2 bytes and reading and writing requires
10.5 $\mu$s and 2.7 $\mu$s, respectively.


\begin{table}[tb]
\begin{center}
\begin{tabular}{|l||r|r|r|r|r|r|} \hline
Term                    & \multicolumn{1}{c|}{BAF}
                        & \multicolumn{1}{c|}{Bytes/}
                        & \multicolumn{1}{c|}{Read}
                        & \multicolumn{1}{c|}{Read/}
                        & \multicolumn{1}{c|}{Write}
                        & \multicolumn{1}{c|}{Write/}\\

                        & 
                        & \multicolumn{1}{c|}{Node}
                        &       
                        & \multicolumn{1}{c|}{Node}
                        &       
                        & \multicolumn{1}{c|}{Node}     \\

                        & \multicolumn{1}{c|}{(bytes)}  
                        &       
                        & \multicolumn{1}{c|}{(s)}   
                        & \multicolumn{1}{c|}{($\mu$s)}         
                        & \multicolumn{1}{c|}{(s)}      
                        & \multicolumn{1}{c|}{($\mu$s)} \\ \hline \hline

\multicolumn{7}{|c|}{Artificial Cases}\\ \hline
Random-unique           &6,073,795& 6.07  & 8.85  & 8.85  & 11.57 & 11.57 \\ \hline
Random                  &567,419 & 0.57  & 2.06  & 2.06  & 2.76  & 2.76  \\ \hline \hline

\multicolumn{7}{|c|}{Real Cases}\\ \hline
COBOL Parse Table   &  370,450 & 0.39  & 0.63  & 0.66  & 1.75  & 1.82  \\ \hline
COBOL System        &2,279,066& 0.07  & 4.88  & 0.16  &20.76  & 0.66  \\ \hline
Risla Library       &  141,946& 0.20  & 0.22  & 0.31  & 0.75  & 1.06  \\ \hline
LPO                 &1,106,661& 0.12  & 1.86  & 0.21  & 9.40  & 1.06  \\ \hline
Casl Specifications &   32,083 & 0.93  & 0.05  & 1.45  & 0.15  & 4.34  \\ \hline
lcc Parse Forest    &  358,318 & 0.99  & 0.34 & 0.99 & 0.95  & 2.77  \\ \hline
S-expressions       & 4,438,229 & 7.47 & 3.31 & 5.57  & 10.49 & 6.23  \\ \hline  
\hline
Real Case Averages  &           & 1.45  &       & 1.32 &      & 2.42 \\ \hline
\end{tabular}
\caption{Reading and writing \ATerms\ as \baf} \label{measurements-baf}
\end{center}
\end{table}

In Table~\ref{measurements-baf} we give results for reading and writing \baf\ files
for the same set of sample terms.
The columns give in order: the size of the \baf\ files in bytes,
the average number of bytes needed per node,
the time to read the \baf\ representation,
the average read time per node,
the time to write the \baf\ representation,
and the average write time per node.
Typically, we can read a node in 1.3 $\mu$s and write it in 2.4 $\mu$s.

Note that reading a \baf\ term is faster than writing the same term,
whereas in case of \ascii\ the writing is faster than reading. This is
caused by the fact that reading the \ascii\ representation of an \aterm\
involves numerous matching operations, whereas reading the \baf\ 
representation can be done with less matching.
On the other hand, writing the \baf\ representation involves more
calculations to encode the sharing of terms, whereas writing 
the \ascii\ representation involves a straightforward term traversal.


\begin{table}[!htb]
\begin{center}
\begin{tabular}{|l||r|r|r|r|r|} \hline

Term                    & \multicolumn{1}{c|}{\ascii}           
                        & \multicolumn{1}{c|}{\baf}             
                        & \multicolumn{1}{c|}{Comp.}    
                        & \multicolumn{1}{c|}{{\tt gzip} }      
                        & \multicolumn{1}{c|}{Comp.}\\

                        & \multicolumn{1}{c|}{(bytes)}          
                        & \multicolumn{1}{c|}{(bytes)}  
                        & \multicolumn{1}{c|}{(\%)}     
                        & \multicolumn{1}{c|}{(bytes)}  
                        & \multicolumn{1}{c|}{(\%)}\\ \hline \hline

\multicolumn{6}{|c|}{Artificial Cases}\\ \hline
Random-unique           & 6,888,889       & 6,073,795       & 11.8          & 2,324,804       & 66.3  \\ \hline
Random                  & 6,199,981       & 567,419        & 90.9          & 439,293        & 92.9  \\ \hline \hline

\multicolumn{6}{|c|}{Real Cases}\\ \hline
COBOL Parse Table       & 4,211,366  &   370,450  & 91.2 &   230,297 & 94.5  \\ \hline
COBOL System            &135,350,005 & 2,279,066  & 98.3 & 3,072,774 & 97.7  \\ \hline
Risla Library           & 2,955,964  &   141,946  & 95.2 &    80,009 & 97.3  \\ \hline
LPO                     &41,227,481  & 1,106,661  & 97.3 &   804,521 & 98.0  \\ \hline
Casl Specifications     &  217,958   &    32,083  & 85.3 &    20,767 & 90.5  \\ \hline
lcc Parse Forest        & 2,244,691  &   358,318  & 84.0 &   244,502 & 89.1  \\ \hline
S-expressions           & 7,954,550  & 4,438,229  & 44.2 & 1,858,366 & 76.6  \\ \hline
\hline
Real Case Averages      &            &            & 85.1 &           & 92.0  \\ \hline
\end{tabular}
\caption{\baf\ \emph{versus} {\tt gzip}} \label{measurements-baf-gzip}
\end{center}
\end{table}


In Table~\ref{measurements-baf-gzip} we show how the compression in \baf\ files
compares to the compression of the standard Unix utility {\tt gzip}.
Considering the same set of examples, we give figures
for a straightforward dump of each term as \ascii\ text (column 1),
the size of the \baf\ version of the same term (column 2)
and percentage of compression achieved (column 3).
Next, we give the results of compressing the \ascii\ version of each term
with {\tt gzip} (column 4), and compression achieved (column 5).
The compression factors are  85\%  for \baf\ and 92\% for {\tt gzip}.
The worst case compression of {\tt gzip} (66\%) is considerably
better than the worst case compression using \baf\ (12\%).
No gains are to be expected from using {\tt gzip} instead of \baf,
since this would imply first writing the \ATerm\ in textual format (an expensive operation which looses sharing)
and then compressing it with {\tt gzip}.

\begin{table}[!htb]
\begin{center}

\begin{tabular}{|l||r|r|r|} \hline

                        & \multicolumn{1}{c|}{Memory}   & \multicolumn{1}{c|}{\ascii}   & \multicolumn{1}{c|}{\baf}\\ \hline \hline
Size per node (bytes)   & 4.51          &  6.20  & 1.45  \\

Read node ($\mu$s)      &               & 10.50  & 1.32 \\
Write node ($\mu$s)     &               &  2.66  & 2.42 \\ \hline
\end{tabular}
\caption{Summary of measurements (based on Real Case averages)} \label{summary}
\end{center}
\end{table}

\subsection{Summary of Measurements}

These measurements are summarized in Table~\ref{summary}.
For in-memory storage, 4.5 bytes are needed per node.
Using \baf, only 1.54 bytes are needed to represent a node.
Also observe that reading \baf\ is an order of magnitude faster than
reading terms in textual form.
In case of parse trees represented as \asfix\ (COBOL System and
Risla Library) less than 2 bytes are needed to represent a node
in memory and less than 2 bits (0.20 bytes) are needed to represent it in
binary format.

%}}}

%{{{ Applications

\section{Applications}  \label{applications}

\ATerms\ have already been used in applications ranging from
development tools for domain specific languages \cite{DK98} to
factories for the renovation of COBOL programs \cite{BSV97}.  The \ATerm\
data type is also the basic data type to represent the terms
manipulated by the rewrite engines generated by the \asdf\ compiler
\cite{BKO99} and they play a central role in the development of the
new \asmetaenv\ \cite{BKMO97}.

\subsection{Representing Syntax Trees: AsFix and CasFix}
\label{asfix-casfix}

The \ATerm\ data type proves to be a powerful and flexible mechanism
to represent syntax trees. By defining an appropriate set
of function symbols parse trees and abstract syntax trees can be
represented for any language or formalism. We describe two examples:
\asfix\ (a parse tree format for \asdf, Section~\ref{asfix})
and CasFix (an abstract syntax tree format for Casl, Section~\ref{casfix}).

\subsubsection{AsFix}
\label{asfix}

\asfix\ (\asdf\ Fixed format) is an incarnation of \ATerms\ for
representing \asdf~\cite{HHKR92.new,BHK89,DHK96}.
\asdf\ is a modular algebraic specification formalism for describing the
syntax and semantics of (programming) languages. \sdf\ (Syntax 
Definition Formalism) allows the definition of the concrete and
abstract syntax of a language and is comparable to (E)BNF.
\asf\ (Algebraic Specification Formalism) allows the definition
of the semantics in terms of equations, which are interpreted
as rewrite rules.
The development of \asdf\ specifications is supported by
an integrated programming environment, 
the \asmetaenv\ \cite{Kli93.meta}.

Using \asfix, each module or term is
represented by its parse tree which contains both the syntax rules
used and all original layout and comments. In this way, the original
source text can be reconstructed from the \asfix\ representation, thus
enabling transformation tools to access and transform comments in the
source text. Since the \asfix\ representation is self-contained (all
grammar information needed to interpret the term is also included),
one can easily develop tools for processing \asfix\ terms which do not
have to consult a common database with grammar information. Examples
of such tools are a (structure) editor or a rewrite engine.

\asfix\ is defined by an appropriate set of function symbols for
representing common constructs in a parse tree.  These function
symbols include the following:

\begin{itemize}
\item $\mbox{\tt prod}(T)$ represents production rule $T$.

\item $\mbox{\tt appl}(T_1,T_2)$ represents
applying production rule $T_1$ to the arguments $T_2$.

\item $\mbox{\tt l}(T)$ represents literal $T$.

\item $\mbox{\tt sort}(T)$ represents sort $T$.

\item  $\mbox{\tt lex}(T_1,T_2)$ represents
(lexical) token $T_1$ of sort  $T_2$.

\item $\mbox{\tt w}(T)$ represents white space $T$.

\item $\mbox{\tt attr}(T)$ represents a single attribute.

\item $\mbox{\tt attrs}(T)$ represents a list of attributes.

\item $\mbox{\tt no-attrs}$ represents an empty list of attributes.


\end{itemize}

The following context-free syntax rules (in \sdf\ \cite{HHKR92.new}) are
necessary to parse the input sentence {\tt true or false}.

\begin{small}
\begin{verbatim}
sort Bool
context-free syntax
  true         -> Bool
  false        -> Bool
  Bool or Bool -> Bool {left}
\end{verbatim}
\end{small}

The parse tree below gives the \asfix\ representation for the input
sentence {\tt true or false}.

\begin{small}
\begin{verbatim}
appl(prod([sort("Bool"),l("or"),sort("Bool")],sort("Bool"),
          attrs([attr("left")])),
     [appl(prod([l("true")],sort("Bool"),no-attrs),[l("true")]),
      w(" "),l("or"),w(" "),
      appl(prod([l("false")],sort("Bool"),no-attrs),[l("false")])
     ])
\end{verbatim}
\end{small}

Two observations can be made about this parse tree.  First,
this parse tree is an ordinary \ATerm, and can be manipulated by all
\ATerm\ utilities in a completely generic way.

Second, this parse tree is completely self-contained and does not
depend on a separate grammar definition.  It is clear that this way of
representing parse trees contains much redundant
information. Therefore, both maximal sharing and \baf\ are essential
to reduce their size. In our measurements,
\asfix\ only plays a role in the cases  COBOL System and Risla Library.

The annotations provided by the \ATerm\ data type can be used to store
auxiliary information like position information derived by the parser
or font and/or color information needed by a (structure) editor.
This information is globally available but can be ignored by tools
that are not interested in it.

\subsubsection{CasFix}
\label{casfix}

Casl (Common Algebraic Specification Language) is a new algebraic
specification formalism \cite{casl1} developed as part of the CoFI
initiative.
It is a general algebraic specification formalism incorporating common
features of most existing algebraic specification languages.  In
addition to the language itself, a set of tools is planned for
supporting the development of Casl specifications.  Existing tools
will be reused as much as possible.

In order to let the various tools, like parsers, editors, rewriters,
and proof checkers, communicate with each other an intermediate format
was needed for Casl. \ATerms\ have been selected as intermediate format
and a specialized version for representing the \emph{abstract} syntax
trees of Casl has been designed (CasFix \cite{BKO98.casl}).  Contrast
this with the approach taken for \asfix, where the more concrete
\emph{parse trees} are used as intermediate representation.

CasFix is obtained by defining an appropriate set of function symbols
for representing Casl's abstract syntax ~\cite{casl1} and by defining
a mapping from Casl's concrete syntax to its abstract syntax.  For
each abstract syntax rule an equivalent CasFix construct is defined
as in:

\begin{small}
\begin{verbatim}
ALTERNATIVE ::= "total-construct" OP-NAME COMPONENTS* 
\end{verbatim}
\end{small}
$\Longrightarrow$
\begin{small}
\begin{verbatim}
total-construct(<OP-NAME>,COMPONENTS*([<COMPONENTS>]))
\end{verbatim}
\end{small}
In this example {\tt "total-construct"} and {\tt "COMPONENTS*"} are
function symbols and {\tt <OP-NAME>} and {\tt <COMPONENTS>} represent
the subtrees of the corresponding sort.

\subsection{ASF+SDF Meta-Environment}

The \asmetaenv\ \cite{Kli93.meta} is an interactive development environment
for writing language specifications in \asdf. 
A new generation of this
environment is being developed based on separate components connected
via the \TB\ \cite{BK98}. 
A description of this new architecture can be found in \cite{BKMO97}.
The new \metaenv\ provides tools for parsing,
compilation, rewriting, debugging, and formatting.
\ATerms\ and \asfix\ play an important role in the new \metaenv:

\begin{itemize}

\item The parser generator~\cite{Vis97} produces a parse table represented as \ATerm.

\item The parser uses this parse table and transforms an input string into
a parse tree which is represented as \asfix\ term.

\item After parsing, the modules of an \asdf\ specification are stored
as \asfix\ terms. Information concerning the specification such as the
rewrite rules that must be compiled are exchanged as \asfix\ terms.

\item The  \asdf\ compiler (see next section) reads and writes \asfix\ terms.

\end{itemize}


\subsection{ASF+SDF to C compiler}
\label{asdf2c}

The \asdf\ to C compiler \cite{BKO99} is a compiler for \asdf.  It
generates ANSI-C code and depends on the \aterm\ library as run-time
environment.  All terms manipulated by the generated C code are
represented as \aterms\ thus taking advantage of maximal subterm
sharing and automatic garbage collection.

The optimized memory usage of \aterms\ has already been exploited in
various industrial projects~\cite{BDKKM96,VanDenBrandKlintVerhoef:98}
where memory usage is a critical success factor.  This \asdf\ compiler
has, for instance, been applied successfully in projects such as the
development of a domain-specific language for describing interest
products (in the financial domain)~\cite{ADR95} and a renovation
factory for restructuring COBOL code \cite{BSV97}.

The \asdf\ compiler is an \asdf\ specification and has been
bootstrapped.  Table \ref{ASF2Ctable} gives some figures on the size
of this specification and the time needed to compile it.  Table
\ref{sharingtable} gives an impression of the effect of compiling the
\asdf\ compiler with and without sharing.  More information on the
compiler itself and on performance issues can be found in
\cite{BKO99}.

\begin{table}[tb]
\begin{center}
\begin{tabular}{|l||c|c|c|c|c|} \hline
Specification   & \asdf\      & \asdf    & Generated            & \asdf\        & C \\
                &             &          &  C code              & compiler      & compiler\\ 
                & (equations) & (lines)  & (lines)              & (sec)         & (sec) \\ \hline \hline
\asdf{}         & 1,876     & 8,699     & 85,185           & 216           & 323 \\
compiler        &          &          &                 &               &  \\ \hline
%%Parser generator & 1388     & 4722     & 47662        & 106           & 192 \\ \hline
%%COBOL formatter  & 2037     & 9205     & 85976        & 208           & 374 \\ \hline
%%Risla expander   & 1082     & 7169     & 46787        & 168           & 531 \\ \hline
\end{tabular}
%\vspace{\baselineskip}
\caption{\label{ASF2Ctable}Some figures on the \asdf\ compiler.}
\end{center}
\end{table}

\begin{table}[tb]
\begin{center}
\begin{tabular}{|l||c|c|} \hline
Application                             & Time (sec)    & Memory (Mb) \\ \hline \hline
\asdf{} compiler (with sharing)         & 216           & \ 16  \\ \hline
\asdf{} compiler (without sharing)      & 661           & 117   \\ \hline 
%%Risla expansion (with sharing)        & \ \ 9         & \ \ 8 \\ \hline
%%Risla expansion (without sharing)     & \ 18          & \ 13  \\ \hline
\end{tabular}

\caption{\label{sharingtable}Performance with and without maximal sharing.}
\end{center}
\end{table}


\subsection{Other Applications}

Other applications are still under development and include:

\begin{itemize}

\item A tool for protocol verification~\cite{GL99}. The \aterms\
are used to represent the states in the state space of the
protocol. Because of the huge amount of states ($\geq 1,000,000$) it
is necessary to share as many states as possible.

\item A tool for the detection of code clones in legacy code.

\item The Stratego compiler \cite{VBT98}.

\end{itemize}

%}}}

%{{{ Discussion

\section{Discussion} \label{discussion}

\subsection{Related Work}
\label{related-aterm}

\paragraph{S-expressions in LISP}
Many intermediate representations are derived in some form or another
from the S-expressions in LISP. \ATerms\ are no exception to this
rule. The main improvements of \ATerms\ over S-expressions are

\begin{itemize}
%% TODO: IS THIS TRUE? \item \ATerms\ are language independent.
\item \ATerms\ support arbitrary binary data (Blobs, see Section~\ref{aterm-datatype}).
\item \ATerms\ support annotations.
\item \ATerms\ support maximal sharing in a systematic way.
\item \ATerms\ support a concise, sharing preserving,
exchange format that exploits the implicit signature
of terms.
\item The \ATerm\ library provides a comprehensive collection of
access functions based on the\emph{ match-and-make} paradigm.
\end{itemize}


\begin{sloppypar}
\paragraph{Intermediate representations in compiler frameworks}
There exist numerous frameworks for compilers and programming
environments that provide facilities for representing intermediate
data. Examples are Centaur's VTP~\cite{BCDIKLP89}, Eli~\cite{Eli92},
Cocktail's Ast~\cite{Ast92}, SUIF~\cite{Suif94}, ASDL~\cite{Wan97}, and
Montana~\cite{Karasick98}. These systems either provide an explicit
intermediate format (Eli, Ast, SUIF) or they provide a programmable
interface to the intermediate data (VTP, Montana, ASDL).  Lamb's
IDL~\cite{Lamb-IDL} and OMG's IDL~\cite{OMG-IDL} are frameworks for
representing intermediate data that are not tied to a specific
compiler construction paradigm but have objectives similar to the
systems already mentioned.
\end{sloppypar}

These approaches typically use a grammar-like definition of the
abstract syntax (including attributes) and provide (generated) access
functions as well as readers and writers for these intermediate data. In most
cases support exists for accessing the intermediate data from a small
collection of source languages.

The major difference between these approaches and \ATerms\ is that they
operate at different levels of abstraction. \ATerms\ just provide the
lower-level representation for terms (or more precisely directed
acyclic graphs), while intermediate representations for compilers are
more specialized and give a higher-level view on the intermediate
data.  They provide primitives for representing program constructs,
symbol tables, flow graphs and other derived information. In most
cases they also provide a fixed format for representing programs at
different levels of abstraction ranging from call graphs to
machine-like instructions. \ATerms\ are thus simpler and more general
and they can be used to represent each of these compiler's
intermediate formats.

Another difference is that most compiler frameworks use a
statically typed intermediate representation.  The major advantage is
early error-detection.  The disadvantages are, however, less
flexibility and the need to generate different access functions for
each different intermediate format. In the case of \ATerms, a dynamic
check may be necessary on the intermediate data but only a single,
generic, set of access functions is needed.

\paragraph{ASDL}

The abstract syntax definition language (ASDL) \cite{Wan97} is a
language for describing tree data structures and is used as
intermediate representation language between the various phases of a
compiler \cite{Han99}.  We consider ASDL in more detail, because of
its public availability and the fact that the goals of
ASDL and \aterms\ are
quite similar as they are both used to exchange of syntax trees 
between tools, although
\aterms\ are more general in the sense that other types of
information, such as unstructured binary objects and annotations, can
also be represented as an \aterm.  Everything that can be represented
by a grammar can be represented in \aterms\ as well as ASDL.

ASDL pickles and the BAF format for \aterms\ are comparable with
respect to functionality, both are binary representations of (among
others) syntax trees.  The pickle and unpickle functions are generated
from the ASDL description and are thus application specific (this may
be more efficient) whereas the reading and writing of BAF is entirely
generic (this avoids the proliferation of versions).


ASDL and \aterms\ can be compared at two different levels:

\begin{itemize}
  
\item \emph{Low level\/:} ASDL pickle versus plain \aterms. By providing an ASDL definition of
  \aterms\ we can compare the size of the same object as \aterm\
  (\ascii\ and BAF) and as ASDL pickle. This is done in Table
  \ref{asdlaterm1} for the COBOL Parse Table. In this case, the
  representation in BAF is an order of magnitude smaller than the ASDL
  pickle.

\begin{table}[tb]
\begin{center}
\begin{tabular}{|l||r|r|r|} \hline
Term                & \ascii  & BAF    & ASDL pickle \\ \hline
COBOL Parse Table & 4,211,366 & 370,450   & 5,262,426 \\ \hline    
\end{tabular}
\caption{Sizes of the COBOL parse table (in bytes)} \label{asdlaterm1}
\end{center}
\end{table}
  
\item \emph{High level}: compare at the level of parse trees or 
abstract syntax trees.  ASDL is typically used to represent abstract
syntax trees while
\aterms\ can be used to represent both as we have discussed in
Section~\ref{asfix-casfix}. To make a meaningful comparison, we
compare 
%%ASDL pickles with \asfix.  By providing an ASDL definition of
%%\asfix\ we can compare the size of the same object in \asfix\ (\ascii\
%%and BAF) and as ASDL pickle. This is done in Table \ref{asdlaterm2}
%%for the parse tree of a COBOL program. In this case, the
%%representation in BAF is 45 times smaller than the ASDL pickle.
%%Another high level comparison is between 
the abstract syntax trees
generated by the lcc back-end in \aterm\ format (both
in \ascii\ and BAF) and the corresponding ASDL pickles.
These figures are presented in Table \ref{asdlaterm3} for the
abstract syntax trees generated for the lcc source files.
In this case the BAF representation is 2 times smaller than
the ASDL pickle. Note that the figure for the BAF representation
differs from the figure in Table \ref{measurements-baf}, this is
caused by the fact that in Table \ref{measurements-baf} all files
are combined into one BAF term whereas in Table \ref{asdlaterm3}
each file is a separate BAF term and their sizes are added.

%%\begin{table}[tb]
%%\begin{center}
%%\begin{tabular}{|l||r|r|r|} \hline
%% Term                   & \ascii  &   BAF & ASDL pickle \\ \hline
%%COBOL program in \asfix & 6778877 & 41087 & 1836850  \\ \hline
%%lcc Parse Forest           &  370742 & 624091 & 1290595   \\ \hline
%%
%%\end{tabular}
%%\caption{Sizes of an \asfix\ term} \label{asdlaterm2}
%%\end{center}
%%\end{table} 

\begin{table}[tb]
\begin{center}
\begin{tabular}{|l||r|r|r|} \hline
Term                   & \ascii  &   BAF & ASDL pickle \\ \hline
lcc Parse Forest           & 2,246,436 & 624,091 & 1,290,595   \\ \hline

\end{tabular}
\caption{Sizes of abstract syntax trees (in bytes)} \label{asdlaterm3}
\end{center}
\end{table} 

\end{itemize} 




\paragraph{XML}
The Extensible Markup Language~\cite{XML1.0} is a recently
standardized format for Web documents. Unlike HTML, XML makes a strict
distinction between \emph{content} and \emph{presentation}.  XML can
be \emph{extended} by adding user-defined \emph{tags} to parts of a
document and by defining the overall structure of the document thus
enabling well-formedness checks on documents.  Although the original
objectives are completely different, there are striking similarities
between \ATerms\ and XML: both serve the representation of
hierarchically structured data and both allow arbitrary extensions
(adding tags \emph{versus} adding function symbols).  There is also a
straightforward translation possible between \ATerms\ and XML.

The main difference between the two is that XML is more verbose and
does not provide a simple mechanism to represent sharing, whereas
\ATerms\ provide the \baf\ format. This may
not be a problem for Web documents like catalogs and database
records, but is does present a major obstacle in our case when we need
to exchange huge terms between tools. We are currently considering
whether some link between \ATerms\ and XML may be advantageous.

\paragraph{Data encodings}

As described in Section~\ref{baf}, we use a form of data encoding to
compress \ATerms\ when they are exchanged between tools.  Of course,
encoding and data compression techniques are in common use in
telecommunications. For instance, the ASN.1 standard gives detailed
rules for data encoding~\cite{ASN.1.PER}.

\begin{sloppypar}
In an earlier project in our group, the Graph Exchange Language 
(GEL)~\cite{Kam94}
has been developed. It is similar in goals to \baf, but \baf\ can
only represent acyclic directed graphs, whereas GEL can represent
arbitrary (potentially cyclic) graphs.  The technical approaches are
different as well. GEL uses a binary-encoded postfix format to
represent the nodes in the graph and introduces explicit labels to
reuse previously constructed parts of the graph. \baf\ uses a prefix
format augmented by generated symbol tables.
\end{sloppypar}

A final difference is in the \emph{usage} of both approaches.  GEL was
used as a separate library that could be used in applications and the
graph encoding was therefore visible to the programmer using
it. \baf\ is, on the other hand, completely integrated in the \ATerm\
implementation and is only used by the standard read and write
functions for \ATerms. The \baf\ format is therefore never visible to
programmers.

\paragraph{Hash consing}

In LISP, the success of hash consing~\cite{Alan:78} has been limited
by the existence of the functions \texttt{rplaca} and \texttt{rplacd}
that can destructively modify a list structure. To support destructive
updates, one has to support two kinds of list structures ``mono copy''
lists with maximal sharing and ``multi copy'' lists without maximal
sharing.  Before destructively changing a mono copy list, it has to be
converted to a multi copy list. In the 1970's, E. Goto has
experimented with a Lisp dialect (HLisp) supporting hash consing and
list types as just sketched.  See~\cite{TerashimaKanada:90} for a
recent overview of this work and its applications.

A striking observation can be made in the context of
SML~\cite{AppelGoncalves:93} where sharing resulted in slightly
increased execution speed and only marginal space savings.  On closer
inspection, we come to the conclusion that both methods for term
sharing are different and can not be compared easily.  We share terms
immediately when they are created: the costs are a table lookup and
the storage needed for the table while the benefits are space savings
due to sharing and a fast equality test (one pointer comparison).
In~\cite{AppelGoncalves:93} sharing of subterms is \emph{only}
determined during garbage collection in order to minimize the overhead
of a table lookup at term creation. This implies that local terms that
have not yet survived one garbage collection are not yet shared thus
loosing most of the benefits (space savings and fast equality test) as
well. 

\subsection{History}

Terms are so simple that most programmers prefer to write their own
implementation rather than using (or even \emph{looking for}) an
existing implementation. This is all right, except when this happens in
a group of cooperating developers as in our case. 

A very first version of the \ATerm\ library was developed as part of
the \TB\ coordination architecture~\cite{BK98}. It was used to
represent data which were transported between tools written in
different languages running on different machines.  Simultaneously, we
were developing a formalism for representing parse 
trees \cite{GB94}.
In addition, incompatible term formats were in use in various of our
compiler projects~\cite{FKW98}. Observing the similarities between all
these incompatible term data types triggered the work on \ATerms\ as
described here. The benefits are twofold. First, a common term data
type is used in more applications and investments in it are well
rewarded. Second, the mere existence of a common data type leads to
new, unanticipated, applications. For instance, we now use \ATerms\ for
representing parse tables as well.

\subsection{Conclusions}

As stated in the introduction, \ATerms\ are intended to form an \emph{open},
\emph{simple}, 
\emph{efficient}, \emph{concise}, and \emph{language
independent} solution for the exchange of (tree-like) data structures
between distributed applications.

\ATerms\ \emph{are} open and language independent since they do not
depend on any specific hardware or software platform.  
\ATerms\ \emph{are} simple: the level one interface consists of only 13
functions.  \ATerms\ \emph{are} efficient and concise as shown by the
measurements in Section~\ref{measurements}.  
%%In encoded form, less than 5 bits are needed per node!  
Last but not least, \ATerms\ are also
\emph{useful} as shown on Section~\ref{applications}.

The \ATerm\ format is supported by a binary exchange format (BAF) which
provides a mechanism to exchange \ATerms\ in a concise way. This
BAF format maintains the in-memory sharing of terms and uses a
minimal amount of bits to represent the nodes, in case of 
\asfix\ terms only 2 bits are needed per node.

The most innovative aspects of \ATerms\ are the simple procedural
interface based on the \emph{make-and-match} paradigm, term
annotations, maximal subterm sharing, and the concise binary encoding
of terms that is completely hidden behind high-level read and write
operations.

%}}}
