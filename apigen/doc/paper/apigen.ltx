%
% File: apigen.ltx
%
% $Id$
%

\documentclass{elsart}

% {{{  packages

\usepackage{verbatim}
\usepackage{epsfig}

% }}}
% {{{  commands

\newcommand{\C}{\mbox{\sc C}}
\newcommand{\adt}{\mbox{\sc adt}}
\newcommand{\afun}{\texttt{AFun}}
\newcommand{\api}{\mbox{\sc api}}
\newcommand{\asdf}{\mbox{\sc Asf+Sdf}}
\newcommand{\asfix}{\mbox{AsFix}}
\newcommand{\asf}{\mbox{\sc Asf}}
\newcommand{\aterms}{\mbox{ATerms}}
\newcommand{\aterm}{\mbox{ATerm}}
\newcommand{\atlib}{\mbox{ATerm-Library}}
\newcommand{\autocode}{\mbox{\sc autocode}}
\newcommand{\java}{\mbox{\sc Java}}
\newcommand{\metaenv}{{\sc Asf+Sdf}\ Meta-En\-vir\-on\-ment}
\newcommand{\sdf}{\mbox{\sc Sdf}}

\newcommand{\heading}[1]{\noindent\textbf{#1}}

% }}}
% {{{  snippet environment

\newenvironment{snippet}{\begin{scriptsize}}{\end{scriptsize}}

% }}}

\begin{document}

% {{{  Frontmatter

\begin{frontmatter}

\title{Generation of \\
  Abstract Programming Interfaces \\
  from Syntax Definitions \\
  $Revision$}

\author{H.A. de Jong \and P.A. Olivier}
\address{
  CWI, Department of Software Engineering, \\
  Kruislaan 413,
  1098 SJ Amsterdam,
  The Netherlands
}

\begin{abstract}
\label{abstract}

Using the \atlib\ to represent tree-like datastructures has become a
popular activity, especially amongst developers of e.g. lexical scanners,
parsers, rewrite engines and model checkers. Practical experience with the
\atlib\ in the \metaenv\ has shown that the development and maintenance
of tools that access \aterms\ using handcrafted code is involved and
very error prone. Both the \emph{make and match} paradigm and the
\emph{direct manipulation} of \aterms\ suffer from the fact that the
programmer uses knowledge about the underlying structure (a.k.a. the signature)
of the \aterm\ that represents the data type being accessed. Hardwiring this knowledge
in various tools makes it difficult to maintain the tools with respect
to changes in the datastructure.

By lifting the data definition and its mapping to an \aterm\
representation away from the level of tool implementation, it becomes
possible to generate a library of access functions on the data type,
which can be used by all the tools that need access to it. The tools no
longer directly manipulate the \aterm\ representation, but rather invoke
methods described by the \api\ of the library, thus abstracting from
the fact that \aterms\ are used for the implementation of the data type.

This paper describes how an \api\ and its implementation can be generated
from a syntax definition of the data type. In particular we describe
how a grammer (in \sdf) can be used to generate a library
of access functions that manipulate the parse trees of terms over this
syntax. Application of this technique in the \metaenv\ has resulted in
a spectacular elimination of 47\% of the handwritten code, thus greatly
improving both maintainability of the tools and their flexibility with
respect to changes in the parse tree format.

\end{abstract}

\end{frontmatter}

% }}}

% {{{  Introduction

\section{Introduction}
\label{sec:intro}

\begin{sloppypar}
Since the development of the \atlib~\cite{BJKO2000} in 1999, its use
for the implementation of tree-like data structures has become quite
popular amongst developers of scanners, parsers, rewrite engines and
model checkers. Apart from its inevitable deployment in the tools of the
\metaenv~\cite{Kli93,BKMO97} for which it was specifically designed,
the \atlib\ is used amongst others in: the ELAN system~\cite{BR01},
The XT Program Transformation Tools~\cite{xt} which are based on the
Stratego Language~\cite{Stratego}, The CoFI Algebraic Specification
Language CASL~\cite{caslsum,CoFI}, and the $\mu$CRL ToolSet for Analysing
Algebraic Specifications~\cite{mucrl-toolset}. \aterms\ include several
nice features: they are easy to manipulate yet very efficient; they
come with a built-in garbage collector (in the \C\ library), and they
have persistency support in the form of a compact, sharing preserving
serialization in both textual and binary representations.
\end{sloppypar}

As more and more tools in the \metaenv\ were converted to work with
the \atlib, it became apparent that the tools had become inflexible with
respect to changes in the parse tree format (called \asfix), and were hard
to maintain. The reason behind this inflexibility was the fact that all
tools used manually encoded structural knowledge about the signature
of the data types, i.e. the location of
data elements inside their \aterm\ representation. Hardwiring such knowledge
into the tools without an explicit signature definition makes it difficult,
if not impossible, to change the \aterm\ representation of the data type. 

The coding practise that uses such structural knowledge is not in any
way restricted to the realm of parse trees. In fact, anyone who has
programmed with the \atlib, will probably be familiar with patterns such
as \texttt{and(<bool>,<bool>)}. And given such a pattern, what could
be easier than writing a function that extracts the left-hand side?
But as these patterns become longer and more intricate with a liberal
sprinkling of quoted strings containing backslash-escaped quotes,
and as they begin to contain lists and annotations, the once so simple
\emph{make-and-match} paradigm becomes a developer's nightmare.

Shielding \aterm\ representation knowledge in access macros somewhat
improves the legibility of code that uses them, but it does not in any way
remove the maintenance issue. It restricts the knowledge to a specific
set of macros, but these still need manual maintenance. As a result,
this approach only \emph{looks like} representation hiding, but in fact
all programmers of different tools still need to know the exact \aterm\
representation of the data being exchanged.

Motivated by the need to change \asfix\ and to avoid the herculean
maintenance task this operation would impose on our toolset, we decided
to remove as much ``\aterm-handicraft'' from the tools as possible
by developing an \api-generator that creates both an interface and an
implementation of data structures represented by \aterms.

While maintaining the advantages of the \atlib\ (in our case most notably
its efficiency due to maximal subterm sharing), applications built with
this generated \api\ benefit from improved simplicity and readability,
they are easier to maintain, and they are more robust against changes
in the underlying \asfix\ representation.

This paper describes how an annotated grammar or syntax definition
can be used to generate a library of functions that provide access to
the parse trees of terms over this grammar. Such a library effectively
turns a parse tree into an abstract data type, providing a typesafe and
systematic API to manipulate terms. In particular we describe
how a \sdf-specification commonly found when using the \metaenv\ is
used to collect the information into an \emph{annotated data type}
(\adt), necessary to build a mapping between grammar productions
and their \aterm-pattern in the underlying \asfix\ parse tree, and
how this mapping is subsequently used to generate \C\ functions that
provide an \api\ to these parse trees. A schematic overview is shown
in Figure~\ref{fig:old-new}.

\begin{figure}[htb]
\centerline{\epsfig{file=old-new.eps,scale=0.8}}
\caption{\label{fig:old-new}Overview of an application before
and after introduction of \api-generation.}
\end{figure}

Although this paper uses tools from the \metaenv\ as a running example,
the maintenance issues addressed here are not specific to parse trees
at all. The issues are fundamental to all applications that use \aterms\
as its data structure representation. Many of these issues are also valid
in applications based on other generic data representation formalisms like
for instance XML.

We first relate our approach to other work in Section~\ref{sec:related},
and continue with some introductory sections on the specification
formalism \asdf\ (Section~\ref{sec:asdf-nutshell}), the syntax
of \aterms\ (Section~\ref{sec:aterm-syntax}), and \asfix\
(Section~\ref{sec:asfix-syntax}). Section~\ref{sec:aterm-access}
subsequently explains how \aterm-based data types are typically
accessed in tools and applications and we show how this approach causes
development and maintenance issues. Section~\ref{sec:syntax-to-api}
describes the actual generation scheme. The results of the
application of our generation technique on tools in the \metaenv\
are shown in Section~\ref{sec:reduction}, followed by some
conclusions (Section~\ref{sec:conclusions}) and future work
(Section~\ref{sec:future}).

% {{{ Related Work

\subsection{Related Work}
\label{sec:related}

We briefly discuss some related work that deals with the aspects we
address in this paper: applying \emph{generational} techniques to create
an \emph{abstraction layer} on top of a \emph{generic} data exchange
formalism.

% {{{  Grammars as Contracts

\medskip 
\noindent\textbf{Grammars as Contracts}
In \cite{gramcon} a generic framework is presented
that includes the generation of libraries from concrete syntax
definitions. These libraries can then be used to manipulate both
parse trees and abstract syntax trees. Just like our work, the
instantiations are based on \sdf\ as syntax definition formalism,
in combination with tool support from the \metaenv. Instantiations
are described for generating libraries in different languages
including C, Java, Stratego, and Haskell.

The work described in our paper can be seen as a refined
instantiation of this generic framework. Among the instantiations
described in \cite{gramcon} generation of a C library
for concrete syntax manipulation is missing, and our approach remedies
this situation. We also focus on generating more intuitive and
readable API's, at the cost of extra annotation effort on the
original syntax definition.

% }}}
% {{{  Zephyr ASDL

\medskip
\noindent\textbf{Zephyr ASDL}

The abstract syntax definition language (ASDL)~\cite{Wan97} is a language
for describing tree data structures much like \aterms, and is used as
intermediate representation language between the various phases of a
compiler~\cite{Han99}.

The ASDL tools support the generation of
accessor and serialization code. The main differences with our approach are:

\begin{itemize}

\item ASDL works on abstract syntax definitions. The link between parser
and ASDL must be programmed manually;

\item ASDL supports a wide variety of languages including C, C++, Java,
Standard ML, and Haskell;

\item ASDL offers a graphical browser and editor for data described
in ASDL;

\item ASDL does not support maximal subterm sharing;

\item There is no garbage collection support for languages like C and C++.

\end{itemize}


% }}}
% {{{  COM/Corba IDL compiler

\medskip
\noindent\textbf{(D)COM/Corba IDL compiler}

The two major commercial component architectures, Microsoft's (D)COM
and OMG's CORBA~\cite{corba,syp98}, both provide IDL compilers that take
an interface definition written in their respective \emph{interface
definition languages} (IDLs), and generate communication scaffolding
code. The generated code includes \emph{stubs} and \emph{skeletons}
to make it easy to write clients and servers respectively.

The biggest difference with our work is that the target of these systems
is to make it easy for programmers to build components in a distributed
setting while we focus on providing an abstraction layer on top of a
generic data exchange format. This means that the IDL compilers generate
code for marshalling arguments when calling remote procedures and for
unmarshalling their return values, while in our approach we keep the data
in the original ``marshalled'' form until it is actually used. In this
sense, our approach could be characterized as lazy and the DCOM/CORBA
approach as eager.

%%\begin{itemize}
%%\item Only generate interfaces not implementations
%%\item Targeted at distributed environments
%%\item Well-defined mappings to specific target languages (whitepapers)
%%\item Interface centric instead of datatype centric
%%\item No sharing
%%\item No garbage collection (in languages without GC)
%%\item (D)COM very windows specific
%%\end{itemize}

% }}}
% {{{  XML access

\medskip
\noindent\textbf{XML data binding in Java}

A comparable approach to provide an abstraction layer on top of a generic
data exchange formalism is used in \texttt{jaxb}~\cite{jaxb}. This is a
tool that generates a Java class hierarchy from an XML DTD. Besides accessor
functions and constructors, (de)serialization functions to and from
XML are generated.  The actual code generation can be steered using a
specification in XML. This makes it possible to add e.g. interfaces and
extra code to the generated classes.

In general this approach is called \emph{data binding}, and several
other initiatives in this area are currently underway, including
some open source initiatives~\cite{databinding.open-source} and
the commercial initiative~\cite{breeze}. All these approaches offers 
tool support for generating
\java\ code from an XML Schema. The generated code can \emph{marshal}
and \emph{unmarshal} XML terms to Java objects with accessors to retrieve
type safe (sub)elements.

% }}}
% {{{  Generative Programming

\medskip
\noindent\textbf{Generative Programming}

Generative programming focuses on using domain engineering to retrieve
domain specific knowledge that can be incorporated into component
generators~\cite{genprog}. At first glance this ``high level'' view on
generating software components seems to be far removed from the low
level view on source code generation we have taken in this paper. If
we take a closer look, the two approaches are not as disjunct as one
might think. We believe any successful generic approach to generative
programming must be based on some abstract data type definition augmented
with domain specific knowledge. In our case, the data type definitions
are written in SDF, and the domain specific knowledge consists of the
mapping of such data types to concrete \asfix\ representations.

% }}}
% {{{  JJForester

\medskip
\noindent\textbf{JJForester}

Another approach to generating code from an SDF definition is taken in
JJForester~\cite{KV2000}. JJForester is a combined parser generator, tree
builder, and visitor generator for Java. The focus lies on the generation
of tree manipulation and especially tree traversal support. Especially
nice is the integration with JJTraveler~\cite{Visser2001a}, providing
generic visitor combinator support.

% }}}

% }}}
% {{{  subsection{\asdf\ in a nutshell}

\subsection{\asdf\ in a nutshell}
\label{sec:asdf-nutshell}

The specification formalism \asdf~\cite{BHK89,HHKR92} is a combination of
the algebraic specification formalism \asf\ and the syntax definition
formalism \sdf. An overview can be found in~\cite{DHK96}. As an
illustration, Figure~\ref{fig:bool-example} presents the definition of
the Boolean data type in \asdf. \asdf\ specifications consist of modules,
where each module has an \sdf-part (defining lexical and context-free
syntax) and an \asf-part (defining equations).

\subsubsection{\sdf}

The \sdf\ part corresponds
to signatures in ordinary algebraic specification formalisms. However,
syntax is not restricted to plain prefix notation but instead arbitrary
context-free grammars can be defined. \sdf\ contains some interesting
features that make it possible to give concise definitions of
context-free grammers:
\begin{itemize}
\item Both context-free and lexical syntax can be specified.
\item Lexical syntax can be described using regular expressions.
\item Associativity can be specified using \emph{attributes} (left, right,
non-assoc).
\item Priority relations between productions can be specified in priority sections.
\item Grammar specifications can be modular.
\item Modules and sorts can be parameterized.
\item A number of heavily used constructs are builtin including lists,
separated lists, alternatives, tuples, and function application.
\end{itemize}

The syntax defined in the \sdf-part
of a module can be used immediately when defining equations, thus making
the syntax used in equations \emph{user-defined}.

% {{{  fig:bool-example

\begin{figure}
\begin{snippet}
\hrulefill
\verbatiminput{bool.example}
\caption{\label{fig:bool-example} \asdf\ specification of the Booleans}
\hrulefill
\end{snippet}
\end{figure}

% }}}

The technology behind SDF is based on \emph{scannerless generalized LR
parsing}. The term \emph{scannerless} indicates that there is no separate scanning
phase before parsing: each character is a token. This approach has the
advantage that the class of languages that can be handled by the parser
is not restricted by local tokenization decisions taken by the scanner.

The term \emph{generalized} means that the parser can handle ambiguous
constructs and in general yields a parse \emph{forest} instead of a single
parse tree.

To implement scannerless parsing for \sdf\, the \sdf\ \emph{normalizer} is used
to transform a \sdf\ grammar into a simple character level grammar.
One of the tasks of the normalizer is to explicitly
insert \emph{layout} symbols between all symbols in context-free syntax
sections. For the syntax defined in Figure~\ref{fig:bool-example} this means
that whitespace can be inserted between keywords, for instance
between {\tt not} and {\tt true} in equation  {\tt not-1}. In this example,
the actual definition of what constitutes whitespace is defined in the
module Layout that is not shown in the example.


\subsubsection{\asf}

The equations appearing in the \asf-part of a specification have the
following distinctive features:

\begin{itemize}
  \item Conditional equations with positive and negative conditions.
  \item Non left-linear equations.
  \item List matching.
  \item Default equations.
\end{itemize}

It is possible to execute specifications by interpreting the equations as
conditional rewrite rules.  The semantics of \asdf\ are based on innermost
rewriting. Default equations are tried when all other applicable equations
have failed, either because the arguments did not match or because one
of the conditions failed.

The development of \asdf\ specifications is supported by an interactive
programming environment, the \metaenv~\cite{tool.demo}. In this
environment specifications can be developed and tested. It provides
syntax-directed editors, a parser generator, and a rewrite engine. Given
this rewrite engine terms can be reduced by interpreting the equations
as rewrite rules. For instance, the term

\begin{quote}
  \texttt{true or false}
\end{quote}

reduces to {\tt true} when applying the equations of
Figure~\ref{fig:bool-example}.

% }}}
% {{{  \subsection{Annotated Terms: the \aterm\ syntax}

\subsection{Annotated Terms: the \aterm\ syntax}
\label{sec:aterm-syntax}

The definition of the concrete syntax of \aterms\ is given in
Appendix~\ref{app:aterm-syntax}. Here are a number of examples to
(re-)familiarize the reader with some of the features of the textual
representation of \aterms:

\begin{itemize}

\item Integer and real constants are written conventionally: \texttt{1},
\texttt{3.14}, and \texttt{-0.7E34} are all valid \aterms.

\item Function applications are represented by a function name followed
by an open parenthesis, a list of arguments separated by commas, and a
closing parenthesis. When there are no arguments, the parentheses may be
omitted. Examples are: \texttt{f(a,b)} and \texttt{"test!"(1,2.1,"Hello
world!")}. These examples show that double quotes can be used to delimit
function names that are not identifiers.

\item Lists are represented by an opening square bracket, a number
of list elements separated by commas and a closing square bracket:
\texttt{[1,2,"abc"]}, \texttt{[]}, and \texttt{[f,g([1,2],x)]} are
examples.

\item A placeholder is represented by an opening angular bracket followed
by a subterm and a closing angular bracket. Examples are: \texttt{<int>},
\texttt{<[3]>}, and \texttt{<f(<int>,<real>)>}.

\end{itemize}

% }}}
% {{{  \subsection{\asdf\ Parse Trees: the \asfix\ syntax}

\subsection{\asdf\ Parse Trees for Dummies: \asfix\ explained}
\label{sec:asfix-syntax}

From a \sdf-specification, a parse table can be generated using the
\texttt{pgen} tool from the \metaenv. \texttt{pgen} consists of the
normalizer discussed earlier combined with a parse table generator.
The resulting parse table can subsequently
be used by \texttt{sglr}\cite{sglr}: the scannerless, generalized LR parser to
parse input terms over the syntax described by the \sdf-specification.
The result of a successful parse is a parse tree. The data structure used
to represent parse trees is called \asfix, and is implemented using the
\atlib\ to exploit the maximal subterm sharing that is commonly present
in parse trees.

Because \asfix\ is a parse tree format (as opposed to an abstract syntax
tree), layout in the input term is preserved, and other syntax-derived
facts such as associativity and constructor information is made available
to any tool that has access to the \asfix\ representation of the input
term.

The definition of the concrete syntax of \asfix\ is given in
Appendix~\ref{app:asfix-syntax}, but to quickly familiarize the reader
with \asfix, we show some of its idiosyncrasies by means of real life
examples.

\hrulefill\nopagebreak

\textbf{Example: grammar production \texttt{"true" -> Bool}}

\nopagebreak
The \asfix\ representation of the \sdf\ production
\begin{quote}
  \verb+"true" -> Bool+
\end{quote}
is:
\begin{verbatim}
  prod([lit("true")],cf(sort("Bool")),no-attrs)
\end{verbatim}
\medskip
The \texttt{prod} symbol declares this to be a grammar production. It
has three arguments: the first is a list of sorts of the elements of
the left-hand side of the production, the second argument is the sort
of the righthand side, and the third argument contains the attributes
(e.g. left associativity) of the production.

In this example, the literal (denoted by the symbol \texttt{lit})
\texttt{true} is the only element in the left-hand side of the
production. It is injected into the context-free (denoted by the symbol
\texttt{cf}) sort \texttt{Bool}. The production has no specific attributes
(\texttt{no-attrs}).


\hrulefill\nopagebreak

\textbf{Example: grammar production \texttt{Bool "and" Bool -> Bool \{left\}}}

\nopagebreak

The \asfix\ representation of the grammar production

\begin{quote}
{\footnotesize\verb+Bool "and" Bool" -> Bool {left}+}
\end{quote}

looks like this:

\begin{snippet}
\begin{verbatim}
 1  prod([cf(sort("Bool")),cf(opt(layout)),lit("and"),cf(opt(layout)),cf(sort("Bool"))],
 2    cf(sort("Bool")),
 3    attrs([assoc("left")]))
\end{verbatim}
\end{snippet}

\begin{itemize}

\item Line 1 declares this to be a grammar production (\texttt{prod}),
containing all the elements of the left-hand side of the production. The
\sdf-normalizer has inserted the context-free sort \texttt{opt(layout)}
subterms at every location where optional layout in the input term
is allowed.

\item Line 2 tells us that the result sort of this production is
\texttt{Bool}.

\item Line 3 shows the attributes associated with this production. In
this case the only attribute is \texttt{left} for left-associativity.

\end{itemize}

\hrulefill\nopagebreak

\textbf{Example: parsed term \texttt{true and false}}

\nopagebreak

If the input term

\begin{quote}
  \texttt{true and false}
\end{quote}

is parsed, the resulting parse tree is the production from the previous
example, applied to the actual argument \texttt{true and false}. The
layout in the input term consists of exactly one space immediately before
and after the keyword \texttt{and}.

\begin{snippet}
\begin{verbatim}
 1 appl(
 2  prod([cf(sort("Bool")),cf(opt(layout)),lit("and"),cf(opt(layout)),cf(sort("Bool"))],
 3       cf(sort("Bool")),attrs([assoc("left")])),
 4 [appl(prod([lit("true")],cf(sort("Bool")),no-attrs),[lit("true")]),
 5  layout([" "]), lit("and"), layout([" "]),
 6  appl(prod([lit("false")],cf(sort("Bool")),no-attrs),[lit("false")])])
\end{verbatim}
\end{snippet}

\begin{itemize}

\item Line 1 states that this tree is the application of a grammar
production to a specific term.

\item Lines 2--3 show the representation of \texttt{Bool "and" Bool ->
Bool \{left\}} from the previous example.

\item Line 4 shows the application of the production \texttt{"true" ->
Bool} to the literal \texttt{true}.

\item Line 5 contains the instantiated optional layout terms. In this
case the input term contained exactly one space immediately before and
just after the keyword \texttt{and}.

\item Similar to line 4, line 6 represents the literal \texttt{"false"}.

\end{itemize}

\hrulefill

The fact that many tools in the \metaenv\ need to operate on such
parse trees, raises the question of how best to access this \aterm\
representation of a data type.

% }}}

% {{{  Accessing \aterm\ Data Types

\section{Accessing \aterm\ Data Types}
\label{sec:aterm-access}

The \atlib\ provides two levels of access to \aterms. We briefly discuss
both of them (Sections~\ref{sec:aterm-lvl1}~and~\ref{sec:aterm-lvl2}) by
showing some examples using the \C\ implementation of the \atlib. Similar
statements are needed when using the \java\ implementation.

Section~\ref{sec:asfix-access} shows the typical way tools in the
\metaenv\ used to access \asfix\ parse trees. As \asfix\ terms are of
impressive complexity to the human eye, the code needed to access them
becomes equally complex if it has to be written down manually.

% {{{  Accessing \aterms\ using the Level One interface

\subsection{Accessing \aterms\ using the Level One interface}
\label{sec:aterm-lvl1}

The first level of access functions is through the easy-to-learn
\emph{make and match} paradigm which allows construction of terms by
parsing their string representation. Placeholders in these patterns
are used to designate ``holes'' in the term which are to be filled in
by other variables, including other \aterms\ as well as native types
(\texttt{int}, \texttt{string}, etc.). Terms are constructed using
\texttt{ATmake}, for example:

% {{{  ATmake example

\begin{footnotesize}
\begin{verbatim}
  ATerm t = ATmake("person(name(<str>),age(<int>))", "Anthony", 7);
\end{verbatim}
\end{footnotesize}

will result in term \texttt{t} being assigned the value:
\begin{quote}
  \texttt{person(name("Anthony"),age(7))}
\end{quote}

% }}}

\noindent Elements from terms can be extracted using \texttt{ATmatch},
for example:

% {{{  ATmatch example

\begin{footnotesize}
\begin{verbatim}
  char *name;
  int age;
  if (ATmatch(t, "person(name(<str>),age(<int>))", &name, &age)) {
    printf("name = %s, age = %d\n", name, age);
  }
\end{verbatim}
\end{footnotesize}

will result in the variables \texttt{name} and \texttt{age} being
assigned the values \texttt{Anthony}, and \texttt{7}, respectively.
The output of this fragment would thus be:

\begin{quote}
  \texttt{name = Anthony, age = 7}
\end{quote}

In case we are only interested in extracting the \texttt{age} field
and we do not care about the actual value of \texttt{name}, we can pass
\texttt{NULL} instead of the address of a local variable. In this case,
that particular subterm is still used during matching, but its actual
value is never assigned. This allows us to test if a specific term matches
a given pattern, without having to bind every placeholder in the pattern.

% }}}

% }}}
% {{{  Accessing \aterms\ using the Level Two interface

\subsection{Accessing \aterms\ using the Level Two interface}
\label{sec:aterm-lvl2}

The second level of access allows more direct manipulation of \aterms\
by means of access-functions which operate directly on a term or its
subterms. This way of access is more efficient than using the level one
interface, because there is no need to parse a string pattern to find
out which part of the (sub-)term is needed.

For example, consider the term from the previous section:
\begin{quote}
  \texttt{t = person(name("Anthony"),age(7))}
\end{quote}

We can get \texttt{Anthony}'s age by first extracting the \texttt{age}
subterm from \texttt{t}, and subsequently getting the actual \texttt{7}
from this \texttt{age} term:

\begin{quote}
  \texttt{int age = ATgetInt(ATgetArgument(ATgetArgument(t, 1), 0));}
\end{quote}

Note that the exact \emph{location} of the \texttt{age} field in the
\aterm\ representation of the \texttt{person} record is used. If the
structure of the record were to change, e.g. a field for the person's
last name is inserted between the \texttt{name} and the \texttt{age}
fields, the example code would be broken.

Also note that this code does not even check if the term \texttt{t}
is of the right form, i.e. if \texttt{t} satisfies the pattern
\texttt{person(name(<str>),age(<int>))}. On an arbitrary input term,
the age-extraction code will most likely fail and dump core. But if only
correct input terms are given, it is the most efficient way to encode
the extraction of the age subterm in this \aterm\ representation of the
\texttt{person} record.

% }}}

\subsection{Accessing \asfix\ parse trees}
\label{sec:asfix-access}

This Section shows several ways in which \asfix\ terms can be
accessed. The code fragments are typical for the way parse trees are
manipulated in the \metaenv. 

First, we show the \C\ code necessary to construct the boolean term
\texttt{true} which, when yielded by the parser, looks like this:

\begin{snippet}
\begin{verbatim}
   appl(prod([lit("true")],cf(sort("Bool")),no-attrs), [lit("true")])
\end{verbatim}
\end{snippet}

Even for such a simple input term, its \aterm\ representation written
as a \C\ (or \java) string is already quite complex. This is because
we have to escape all the double quotes (the \verb+"+ characters) from
interpretation by the compiler:

% {{{  Parse Tree for "true"

\begin{snippet}
\begin{verbatim}
 ATerm true = ATparse(
   "appl(prod([lit(\"true\")],cf(sort(\"Bool\")),no-attrs),[lit(\"true\")])");
\end{verbatim}
\end{snippet}

% }}}

As another example, consider a \C\ function that extracts the left-hand
side from a boolean conjunction. It needs to match the parse tree
of the incoming term against the pattern for the syntax production:

\begin{quote}
  \texttt{Bool "and" Bool -> Bool \{left\}}
\end{quote}

An implementation using the level one interface would need the pattern
written as a string, with a \texttt{<term>} placeholder at the correct
spot. Because the pattern is written inside a string, we once again
need to escape all quotes.  Moreover, the string representation of the
match-pattern is so long that it does not legibly fit on one line anymore,
and we need to resort to ANSI \C\ string concatenation\footnote{Strings
can be split over multiple lines by ending one line with a \texttt{"}
and starting the next line with another \texttt{"}.} to span the string
over multiple lines.

% {{{  Level One: Extracting lhs from "Bool and Bool -> Bool"

\begin{snippet}
\begin{verbatim}
  ATerm extract_bool_lhs(ATerm t) {
    ATerm lhs;
    char *bool_and_lhs_pattern = 
      "appl(prod([cf(sort(\"Bool\")),cf(opt(layout)),lit(\"and\"),cf(opt(layout)),"
      "cf(sort(\"Bool\"))],cf(sort(\"Bool\")),attrs([assoc(\"left\")])),"
      "[<term>,<term>,lit("and"),<term>,<term>])";
  
    if (ATmatch(t, bool_and_lhs_pattern, &lhs, NULL, NULL, NULL)) {
      return lhs;
    }

    return NULL;
  }
\end{verbatim}
\end{snippet}

% }}}

Could there be a quote missing in the pattern? Are all the \verb+)+,
\verb+]+, and \verb+}+ characters where they should be? Did you expect
\emph{four} \texttt{<term>} placeholders in the pattern (to account for
the lhs, the rhs, as well as the optional layout before and after the
literal \texttt{and})?

Keep in mind that:

\begin{itemize}

\item as long as it is a valid \C\ string, the \C\ compiler is not going
to warn you if you make a mistake (e.g. you wrote \texttt{lit(and)}
instead of \verb+lit(\"and\")+);

\item as long as it is a valid \aterm-pattern, the \aterm\ parser is
not going to warn you if you make a mistake (e.g. you forgot to add
\texttt{<term>} placeholders for the optional layout);

\item if you made any mistakes, your only hope to fix them lies in
visually inspecting the incoming term and the expected matching pattern,
and figuring out why they do not match!

\end{itemize}

An implementation using the level two interface encodes structural
knowledge about the exact location of the \texttt{lhs} in terms of direct
\aterm\ access functions. In particular, recalling that in \asfix\ we
are dealing with \texttt{appl(prod,[args])} patterns, the \texttt{args}
are always the second argument of the \texttt{appl}. If we look closely
at the \asfix\ pattern for our \texttt{and}-terms, we notice that the
\texttt{lhs} is the first element in this list of \texttt{args}. The
extraction function can thus be simplified to the more efficient, but
very type-unsafe and obfuscated:

% {{{  Level Two: Extracting lhs from "true and false"

\begin{snippet}
\begin{verbatim}
  ATerm extract_bool_lhs(ATerm t) {
    ATermList args = ATgetArgument(t, 1);  // get arguments from AsFix "appl"
    return ATgetFirst(args);               // lhs is the first of these args.
  }
\end{verbatim}
\end{snippet}

% }}}

After all, this function would work on any \aterm\ function application
that has (at least) two arguments, the first of which is a list with
(at least) one element.

% {{{  Maintenance issues

\subsection{Maintenance issues}
\label{sec:maintenance}

There are several fundamental maintenance issues inherent in the use
of \aterms\ as a data structure implementation in hand-crafted tools.

\begin{itemize}

\item The esoteric art of writing down multi-line, quote-escaped string
patterns and the subsequent substitution of parts of these patterns to
contain the desired placeholders at the correct locations, is so error
prone that it is almost guaranteed to go wrong at some point. Practical
experience in the \metaenv\ has proven this many times over. Handcrafted
\aterm-patterns proliferate through numerous versions of various tools,
and after a while all sorts of ``mysterious'' bugs creep up where one tool
cannot handle the output of another tool, or simply bails out reporting
that deep down some part of an input term does not satisfy a particular
assertion. Obviously, these errors are often due to pattern mismatches,
misplaced placeholders, or ill-escaped quotes.

\item Even if the patterns are written down correctly, or when the Level
Two interface is used (which doesn't use \aterm-patterns), there is much
work to be done when the application syntax changes.

Suppose for example that we want to change the syntax of our boolean
conjunction from infix notation:

\begin{quote}
  \texttt{Bool "and" Bool -> Bool}
\end{quote}

\noindent
into prefix notation:

\begin{quote}
  \texttt{"and" "(" Bool "," Bool ")" -> Bool}
\end{quote}

\noindent Conceptually nothing has changed: we mean exactly the same
arguments when we address them as \texttt{lhs}, \texttt{rhs}, and
\texttt{result} terms in both productions. However, in the underlying
parse tree the location of \emph{all three} subterms has changed! This
in turn means that all tools that manipulate, e.g. the \texttt{lhs}
of boolean terms, have to be updated to reflect this structural change.

In fact, there is hardly any room for flexibility with respect to changes
in the syntax, unless the arguments happen to remain at their original
position. Every tool based on the modified \emph{application syntax}
has to be updated.

\item With such inflexibility with respect to the application syntax
in mind, imagine what would happen if the structure of the parse
trees (\asfix) \emph{itself} were to change! Every tool based on
the \emph{representation} of parse trees would have to be updated to
reflect the structural changes in the format. In our practical case of
the \metaenv\ where we wanted to rid \asfix\ of some legacy constructs,
this meant modification of virtually \emph{every} tool --- an arduous
task indeed!

\end{itemize}

% }}}

% }}}
% {{{  From syntax to \api

\section{From syntax to \api}
\label{sec:syntax-to-api}

Abstracting from implementation details about the facts that there
is such a thing as a parse tree format and that this format in turn
is implemented using \aterms, it is easy to name several operations a
toolbuilder would like, given a syntax definition.

% {{{  What you want to be able to do, given an SDF definition

As an example we consider the booleans again. Some of the typical things
a toolbuilder would like to be able to do given the boolean syntax are:

\begin{itemize}

\item Use a type definition for booleans (it is better to have a specific
type \texttt{Bool} than to use the generic \texttt{ATerm} type);

\item Create the basic booleans: \texttt{true} and \texttt{false};

\item Create a compound boolean term using basic and other compound
boolean terms;

\item Given an arbitrary term, test if it is a valid boolean term;

\item Given an arbitrary boolean term, distinguish between a basic
term and a compound term, e.g. by testing if it has a \texttt{lhs}
or \texttt{rhs};

\item Extract the \texttt{lhs} and \texttt{rhs} of a given boolean term;

\item Replace the \texttt{lhs} and \texttt{rhs} of a compound boolean
term by another boolean term;

\item etc.

\end{itemize}

% }}}

Obviously, this list is not exhaustive, but it does form a nice starting
point. Fortunately, all the necessary information can be extracted
from an \sdf-definition of the grammar. In order to separate some
concerns and simplify the generation framework, we split the process
into two steps (see Figure~\ref{fig:gen}). First, we extract all the
necessary information from the \sdf-definition, and store it in a
convenient format. This step
takes care of the parsing and analysis of the grammar.
The second step takes the intermediate format and does the
actual generation for a specific target language.
\begin{figure}[htb]
\centerline{\epsfig{file=gen.eps,scale=1.3}}
\caption{\label{fig:gen}Generation scheme: from \sdf\ to \adt\ to code}
\end{figure}

We call the intermediate format \emph{annotated data type}, or \adt\
for short.

 It holds the minimal amount of information for each syntax
rule in the original \sdf\ specification. In particular, for each rule
we need:

% {{{  The necessary elements of an adt-entry

\begin{itemize}

\item The \emph{sortname} of the production. In our boolean syntax this
is \texttt{Bool};

\item The \emph{alternative} of the production. Our boolean syntax
has five alternatives: \texttt{true}, \texttt{false}, \texttt{not},
\texttt{and}, \texttt{or}.

\item The actual \aterm-\emph{pattern} representation of the rule.
In this pattern, each \emph{field} (non-terminal in the syntax rule)
is replaced by a typed placeholder containing the \emph{sort} of the
non-terminal and a \emph{descriptive name}. For the \texttt{and} rule
we could use \texttt{lhs}, and \texttt{rhs}, both of type \texttt{Bool}.

\end{itemize}

% }}}

Since we are solving the maintenance problem of using \aterms\ as a
data type representation, we decided we could very well use an \aterm\
to represent the elements of an \adt. The obvious advantage is that
we get persistency (saving and loading of an \adt) for free, and we do
not need to construct a domain specific language (with its own parser
etc.) which would introduce undesired development-time overhead. Each
entry in the \adt\ consists of the three elements \emph{sortname},
\emph{alternative}, and \emph{term-pattern}, which we can easily
represent as an \aterm-list.  An entire \adt\ consists of nothing more
than a list of such lists. Instead of using a list, each single entry
could also have been represented as a function with three arguments,
but we opted for as little syntactic sugar in the entries as possible,
to simplify development-time debugging.  Remember that an \adt\ entry
contains an \aterm\ pattern and they are hard enough to read, without
the introduction of an extra function-symbol around them.


% {{{  Example adt-entry with explanation

As an example of the concrete representation of an \adt\ entry, let
us look at the boolean \texttt{and}. In this example, we know that the
sortname of the production is \texttt{Bool}, the alternative is called
\texttt{and}.  There are two operands, \texttt{lhs} and \texttt{rhs},
both of type \texttt{Bool}. In the pattern we put typed placeholders
\texttt{<lhs(Bool)>} and \texttt{<rhs(Bool)>} at the location of
the corresponding non-terminals. Also, because this is a parse tree
pattern, we have to allow layout (whitespace), which in this case can
occur both after the non-terminal \texttt{lhs}, and after the literal
\texttt{and}. The \adt\ entry thus becomes:

\begin{snippet}
\begin{verbatim}
 1  [Bool,
 2   and,
 3   appl(prod(
 4    [cf(sort("Bool")),cf(opt(layout)),lit("and"),cf(opt(layout)),cf(sort("Bool"))],
 5     cf(sort("Bool")),attrs([assoc(left)])),
 6    [<lhs(Bool)>,<ws-after-lhs(Layout)>,lit("and"),<ws-after-and(Layout)>,
 7     <rhs(Bool)>])]
\end{verbatim}
\end{snippet}

Lines 1 and 2 contain the sortname and the alternative, respectively.
Following, in lines 3--7 is the \aterm\ pattern of the actual parse
tree. Lines 3--5 show the \texttt{prod} of the \asfix\ function
application.  Lines 6--7 show the \texttt{args} part. Clearly visible
are the typed placeholders for \texttt{lhs} and \texttt{rhs}.

The two placeholders matching optional layout have the somewhat
arbitrary names \texttt{ws-after-lhs}, and \texttt{ws-after-and}.
Section~\ref{sec:sdf-to-adt} elaborates on the naming schemes used to
generate legible, understandable names.

% }}}

Given an \adt, which is generated from an \sdf\ definition,
but which could also come from any other source, we no longer need to
worry about any \sdf\ peculiarities, or parse tree specifics. Instead,
we can concentrate on generating the desired functionality for a given
target language. In this paper we concentrate on describing the steps
needed to produce legible, type-safe \C\ code. Optimizations to the
generated code can easily be obtained by removing type-safety checks,
resulting in a more efficient production version of the code.

% {{{  Deriving the \adt\ from a \sdf\ specification

\subsection{Deriving the \adt\ from a \sdf\ specification}
\label{sec:sdf-to-adt}

Now that we know what specific information we need in the \adt, how do we
get it from the \sdf\ definition? If we look back at our \sdf\ definition
of the booleans, we can derive two of the necessary elements immediately:

\begin{itemize}

\item The result \emph{sort} of a syntax rule. It is explicitly mentioned
at the end of each rule.

\item The \aterm\ pattern. It can be constructed by following the exact
same rules for constructing \asfix\ terms that the \sdf\ normalizer uses.

\end{itemize}

This leaves us with the issue of coming up with a decent name for each
\emph{alternative} production of the same sort, and we still need to
figure out a way to give \emph{descriptive} names to the non-terminals
in the grammar rule.

\noindent\textbf{Naming the non-terminals}

Given our \sdf\ rule for the boolean \texttt{and}, can we derive a
sensible name for each of the \texttt{Bool} non-terminals? The only
information we have is our syntax rule:

\begin{quote}
{\small\texttt{Bool "and" Bool -> Bool \{left\}} }
\end{quote}

If we use heuristics to call them e.g. \texttt{lhs} and \texttt{rhs},
what do we do when we find another syntax rule that has three, four
or even more arguments? In syntax rules with only one non-terminal,
we could default to using the sort name of that non-terminal. But in
general, it is hard to come up with any kind of descriptive naming scheme.
Keep in mind that most toolbuilders will not really be happy if they are
confronted with access functions that have arbitrarily complex names,
or numbered arguments.

Instead of coming up with any kind of heuristic at all, we opted to
use the \emph{labeling} mechanism present in \sdf, which allows grammar
writers to label each non-terminal. This eliminates the need to invent a
descriptive name altogether and provides an understandable link between
items in a grammar rule and their generated access functions. Suppose
we like the abbreviations \texttt{lhs}, and \texttt{rhs}, we could label
the syntax rule for \texttt{and} to become:

\begin{quote}
{\small \texttt{lhs:Bool "and" rhs:Bool -> Bool \{left\}} }
\end{quote}

\noindent\textbf{Naming the alternatives}

Similarly, we need a solution for the \emph{alternative} name. In
this case the literal \texttt{and} happens to be a name we could use.
But what if there is no literal at all? Or if there are multiple literals
in a production, which one should we pick? Should they be concatenated?
What if the literal is some sort of baroque lexical expression (think of
the \C\ and \java\ symbols \verb+&&+ for conjunction). Again we are saved
by \sdf, which provides a way to annotate syntax rules. In fact, we re-use
an annotation which is quite commonly used by \sdf\ syntax writers to
annotate the name of the \emph{abstract syntax} node that corresponds to
this particular syntax rule. Traditionally the \texttt{cons} annotation is
used for this purpose.  So, finally our \texttt{and} syntax rule becomes:

\begin{quote}
{\small \texttt{lhs:Bool "and" rhs:Bool -> Bool \{left, cons("and")\}} }
\end{quote}

From which we can subsequently generate (e.g. \C) type and function
names as shown in Figure~\ref{fig:names}.

\begin{figure}[htb]
\epsfig{file=names.eps}
\caption{\label{fig:names}Using \sdf\ elements to derive legible names.}
\end{figure}

% }}}
% {{{  Code generation from \adt\ to \C

\subsection{Code generation from \adt\ to \C}
\label{sec:adt-to-c}


% {{{  Generated types and functions

\subsection{Generated types and functions}
For each sortname in an \adt, we generate the following items
(which are further explained in Subsection \ref{sec:implementation}):

\begin{itemize}

\item An opaque type definition to distinguish instances of this
particular sort from other \aterms.

\item Conversion functions \texttt{fromTerm} and \texttt{toTerm}
to interface with generic \aterm\ functions, such as
\texttt{ATreadFromFile}. These functions perform a type cast, and as
such they form the entry and exit points to type-safety.

\item A validity function to test whether an instance of a sort is indeed
valid, i.e. that it indeed matches one of the \aterm-patterns defined
as an alternative of this sort. This is useful to assert the validity
of an externally acquired instance of this sort, e.g. if it has just
been read from file.

\item Constructor functions for each possible alternative for this sort
to create instances from scratch.

\item An equality function to test equality with another instance of
this sort.

\item For each alternative of the sort, an \texttt{isAlternative}
function that checks if the current object is an instance of that
particular alternative.

\item For each field used in any of the alternatives of the sort,
a \texttt{hasField} function that checks if the current object is an
instance of an alternative that has that non-terminal.

\item Similarly, a \texttt{getField} and \texttt{setField} method for
each of the fields in a sort.

\end{itemize}

% }}}
% {{{  Implementation

\subsection{Implementation}
\label{sec:implementation}

In order to address the maintenance issues associated with the
proliferation of \aterm-patterns, we decided it was a good idea to
isolate them as much as possible from the actual code. This is achieved
by generating a separate \emph{dictionary} file containing all the
\aterm\ patterns used by the library. This \emph{dictionary} file
declares a separate \afun\ variable for each \aterm\ function symbol,
and an \aterm\ variable for each possible pattern. An initialization
function is also generated which takes care of the proper initialization
of all these variables, and the necessary calls to \texttt{ATprotect}
to shield them from the built-in garbage collector. A verbatim dump of
all the patterns is included in a comment section in the generated code,
to provide debugging feedback when necessary. An example of a dictionary
file can be found in Appendix~\ref{app:bool_and_dict}.

The actual implementation of the \api\ functions is generated in its
own \C\ file, accompanied by a header file containing the signatures of
all exported \api\ functions. We show abridged snippets of the generated
code. The header file is straightforward, containing merely the opaque
type definition, and the declarations of the functions contained in the
\C\ file.

% {{{  Opaque type definition

\heading{Opaque type definition}

Defining \verb+Bool+ to be a pointer to a non-existant type (in this case
\texttt{struct \_Bool}, hides the underlying \aterm\ representation from
the point of view of \api\ users. Instances of \texttt{Bool} can safely
be passed around by functions, but any attempt to dereference such a
pointer results in a compile time error.

\begin{snippet}
\begin{verbatim}
  typedef struct _Bool *Bool;
\end{verbatim}
\end{snippet}

% }}}
% {{{  Term convertors

\heading{Term convertors}

These functions perform no real operation, but take care of the
type casting between the generic \aterm\ type and the more specific
\texttt{Bool}.  They are needed as entry and exit points to type-safety
when \atlib\ functions such as \texttt{ATreadFromFile} are used, which
yield an \aterm.

\begin{snippet}
\begin{verbatim}
  Bool BoolFromTerm(ATerm t) { return (Bool)t; }
  ATerm BoolToTerm(Bool arg) { return (ATerm)arg; }
\end{verbatim}
\end{snippet}

For improved efficiency, these functions could easily be replaced by
macros which perform the exact same type cast. Unfortunately, this
irrevocably kills type-safety, because macros are expanded during the
pre-processor phase, without any form of type checking on the arguments
of the macro.

% }}}
% {{{  Equality test

\heading{Equality test}

Because \aterms\ are used as implementation, we get the trivial equality
check based on memory address comparison for free. We only need to
provide a type-safe wrapper around \texttt{ATisEqual}.

\begin{snippet}
\begin{verbatim}
  ATbool isEqualBool(Bool arg0, Bool arg1) {
    return ATisEqual((ATerm)arg0, (ATerm)arg1);
  }
\end{verbatim}
\end{snippet}

As with the convertor functions, the equality function can be replaced by
a macro definition (with the same concerns about the loss of type-safety)
for improved efficiency.

% }}}
% {{{  Validity test

\heading{Validity test}

Whenever an \aterm\ is acquired from an external source (e.g. by
reading it from file) and is converted to \texttt{Bool}, programmers
might like to assert that the term satisfies one of the alternatives for
\texttt{Bool}. After all, any valid \aterm\ will happily be parsed by
\texttt{ATreadFromFile} and subsequent conversion by \texttt{TermToBool}
is done without any verification. The \texttt{isValidBool} function
checks whether a given \texttt{Bool} argument is indeed an instance of
one of the correct alternatives.

\begin{snippet}
\begin{verbatim}
  ATbool isValidBool(Bool arg) {
    if (isBoolTrue(arg)) { return ATtrue; }
    else if (isBoolFalse(arg)) { return ATtrue; }
    else if (isBoolNot(arg)) { return ATtrue; }
    else if (isBoolAnd(arg)) { return ATtrue; }
    else if (isBoolOr(arg)) { return ATtrue; }
    return ATfalse;
}
\end{verbatim}
\end{snippet}

As checking the alternatives is expensive, the conversion functions
themselves do not directly invoke \texttt{isValidBool}. Efficiency
of the \texttt{BoolToTerm} function could be traded for even more
robustness, by making it refuse to convert any \aterm\ that does not
satisfy \texttt{isValidBool}.

Also note that the \texttt{isBoolX} functions perform a \emph{shallow}
match: they do not check the \emph{arguments} of the alternative they
test. For example, \texttt{isBoolAnd} does not check if its \texttt{lhs}
and \texttt{rhs} are actually valid booleans. It merely tests if the
term is an instance of the pattern for the \texttt{and} alternative.
It would be possible to generate code that performs a \emph{deep} match,
again at the cost of a considerable efficiency hit.

% }}}
% {{{  Inspector

\heading{Inspector}

Inspecting a \texttt{Bool} to see if it is an instance of a specific
alternative involves matching the argument against the pattern for
that particular alternative. Because matching is (very) expensive,
the result of the most recent match is cached. This caching approach
seems limited, but is useful when multiple subterms of the \emph{same
argument} are accessed. In these cases, sequences of \texttt{getBoolX}
and \texttt{setBoolY} all reuse (cached) inspection results.

\begin{snippet}
\begin{verbatim}
  ATbool isBoolTrue(Bool arg) {
    static ATerm cached_arg = NULL;
    static int last_gc = -1;
    static ATbool cached_result;

    assert(arg != NULL);

    if (last_gc != ATgetGCCount() || (ATerm)arg != cached_arg) {
      cached_arg = (ATerm)arg;
      cached_result = ATmatchTerm((ATerm)arg, patternBoolTrue);
      last_gc = ATgetGCCount();
    }

    return cached_result;
  }
\end{verbatim}
\end{snippet}

Note that the cached \aterm\ is deliberately \emph{not} protected from
garbage collection. Doing so would result in all inspector functions
holding on to references of \aterms that could not be collected. These
terms are potentially very large and the memory behaviour would become
extremely unpredictable. We therefore opted for a solution where caching
results are only valid \emph{until the next garbage collection}.
This is done by comparing the current garbage collection count with
the same count at the time the cached result was calculated.


% }}}
% {{{  Query accessor

\heading{Query accessor}

The query accessor checks if a given argument has a specific field. It
is implemented by checking if the argument is an instance of any of the
alternatives which has the required field.

\begin{snippet}
\begin{verbatim}
  ATbool hasBoolLhs(Bool arg) {
    if (isBoolAnd(arg)) { return ATtrue; }
    else if (isBoolOr(arg)) { return ATtrue; }
    return ATfalse;
  }
\end{verbatim}
\end{snippet}

% }}}
% {{{  Get accessor

\heading{Get accessor}

The \texttt{get}ter is implemented much like the query accessor. It
inspects the incoming argument to find out of which alternative it is
an instance.  Upon finding the right alternative, it returns the intended
subterm by directly peeking into the \aterm\ representation.

If the production has but a single alternative, no testing is needed
and the requested subterm can be returned immediately.

\begin{snippet}
\begin{verbatim}
  Bool getBoolArg(Bool arg) {
    return (Bool)ATelementAt((ATermList)ATgetArgument((ATermAppl)arg, 1), 2);
  }
\end{verbatim}
\end{snippet}

If there are multiple alternatives, each is tested in turn, until a
single alternative remains, which must be the right one (since none of
the other alternatives matched, and we assume a valid instance of one
of the alternative productions).

\begin{snippet}
\begin{verbatim}
  Bool getBoolLhs(Bool arg) {
    if (isBoolAnd(arg)) {
      return (Bool)ATgetFirst((ATermList)ATgetArgument((ATermAppl)arg, 1));
    }
    else 
      return (Bool)ATgetFirst((ATermList)ATgetArgument((ATermAppl)arg, 1));
  }
\end{verbatim}
\end{snippet}

An obvious optimization is to detect if there are alternatives that
have the requested field at the same location in the underlying \aterm\
representation. In this case, the \texttt{isBoolAnd} test is redundant,
because both alternatives of \texttt{Bool} that have a \texttt{lhs},
have it at the exact same position. The condensed version would look
much like the previous \texttt{getBoolArg} and would be much cheaper
since it does not have to do any matching:

\begin{snippet}
\begin{verbatim}
Condensed code for alternatives: "and", "or"
  Bool getBoolLhs(Bool arg) {
    return (Bool)ATgetFirst((ATermList)ATgetArgument((ATermAppl)arg, 1));
  }
\end{verbatim}
\end{snippet}

% }}}
% {{{  Set accessor

\heading{Set accessor}

The implementation of the \texttt{set}ter is again along the same path
as the \texttt{get}ter and the inspector. The main issue here stems from
the fact that \aterms\ are immutable. Consequently, all \texttt{set}ters
need to be of a functional nature. This means that they cannot update
the \aterm\ \emph{in situ}, but instead need to construct a \emph{new}
\aterm, reflecting the desired change.

\begin{snippet}
\begin{verbatim}
  Bool setBoolLhs(Bool arg, Bool lhs) {
    if (isBoolAnd(arg)) {
      return (Bool)
        ATsetArgument((ATermAppl)arg,
                      (ATerm)ATreplace((ATermList)ATgetArgument((ATermAppl)arg, 1),
                                       (ATerm)lhs, 0), 1);
    }
    else if (isBoolOr(arg)) {
      return (Bool)
        ATsetArgument((ATermAppl)arg,
                      (ATerm)ATreplace((ATermList)ATgetArgument((ATermAppl)arg, 1),
                                       (ATerm)lhs, 0), 1);
    }
  
    ATabort("Bool has no Lhs: %t\n", arg);
    return (Bool)NULL;
  }
\end{verbatim}
\end{snippet}

As the construction of a new \aterm\ is very expensive to begin with,
the gain of eliminating the test for one of the alternatives (as
implemented in the \texttt{get}ters) is minimal, which is why that
particular optimization is omitted here. If no match was found after
exhaustively testing all the alternatives, the operation is aborted.

% }}}

% }}}

% }}}

% }}}
% {{{  Code reduction in the \metaenv

\section{Software engineering benefits in the \metaenv}
\label{sec:reduction}

Our main motivation to start this work has been the desire to make changes
to \asfix, the parse tree format used by our tools in the \metaenv. Of
particular interest is the dramatic size reduction (in terms of lines
of code) of the various tools after refactoring them to use the new \api s.

This \emph{apification} process consisted of weeding out any and all
direct \aterm\ manipulations from the tools, replacing them by calls
to their \api\ counterparts. The resulting code is much easier to read,
understand and maintain by programmers, whereas before they could only
be handled by \asfix\ guru's.  In accordance with these subjective
\emph{feelings} that the code had improved, is the LOC (Lines of
Code) metric. Comparing versions just before and immediately after
\emph{apification}, we found out that we had been able to eliminate
almost half of the (manually written) code. The LOC metrics have been
summarized in Figure~\ref{fig:reduction}.

\begin{figure}[htb]
\begin{tabular}{|l|r|r|r|}
\hline
Component     & Before (LOC)  & After (LOC) & Reduction (\%) \\
\hline
asc-support   &		2207  &	      1752  &	21	\\
libasfix      &	       10419  &	      2077  &	80	\\
asfix-tools   &		 466  &	       603  &  -29	\\
asfix2c compiler &	1866  &	      1138  &	39	\\
asf-tools     &		1303  &	       589  &	55	\\
structure editor &	2861  &	      1946  &	32	\\
evaluator     &		4241  &	      4009  &	 5	\\
module-db     &		1809  &	      1244  &	31	\\
\hline
{\bf Total }  &  \textbf{25172} &	\textbf{13358}	    & \textbf{47} \\
\hline

%% in-output     &		 713  &	       812  &  -14	\\
%% pgen	      &		5226  &	      5226  &	 0	\\
%% sglr	      &	       10225  &	     10427  &	-2	\\
%% utils	      &		1356  &	      1363  &	-1	\\
%% \hline
%% \hline
%% \textbf{Total}	      &	       \textbf{42692}  & \textbf{31186}  & \textbf{27} \\ 
%% \hline

\end{tabular}
\caption{\label{fig:reduction}Code Reduction}
\end{figure}

Understandably the biggest gain was achieved in {\tt libasfix}, because
most of this library is now generated from the SDF definition of SDF itself.
Only some high level functionality that could not be generated remains in
this library.

Somewhat more subjective but still quite spectacular is the fact that
after apification, we where able to make significant changes to the
\asfix\ format itself and implement these changes in all our tools
in a matter of days. Similar changes that where implemented before
apification took weeks of extensive coding and painstaking
debugging sessions to find all affected locations.

% }}}
% {{{  Conclusions

\section{Conclusions}
\label{sec:conclusions}

Generating access libraries from \sdf\ definitions offers a simple,
consistent way of developing and maintaining type-safe, efficient
data types.

The application in the \metaenv\ of the techniques described in this
paper resulted in the elimination of a significant portion of handcrafted
code. The effect of this elimination is amplified by the inherent nature
of the affected code portions: hard to read and write, difficult to
maintain, and in general very error prone to handle at all.

The result of our generational approach is a type-safe replacement for
manually crafted libraries that provide access to compound data types
implemented by \aterms. Even though several optimization opportunities
have not yet been fully explored, the efficiency of the generated library
is already comparable to its manually written predecessor.

% }}}

% {{{  Discussion

\section{Discussion}

In retrospect, one might ask why on earth we began developing code using
direct \aterm\ manipulation in the first place. The answer lies partially
in the power and addictiveness of working with \aterms. Because it is
so utterly simple to write a tool that uses simple \aterm\ patterns,
several developers quickly started writing their own applications. Later,
when some of the tools demonstrated a need for speed, parts of the now
grown-but-not-restructured tools were rewritten to use the more efficient
level two interface instead of the matching interface, mostly in the
form of ad hoc restructuring, driven by the output of the \texttt{gprof}
profiler. When prompted to implement changes in our parse tree format,
we realized the era of direct \aterm\ manipulation had to end, and we
had to find a structural solution to representing data types by \aterms,
or we would be unable to maintain our toolset. Fortunately, the road of
generating the access library as described in this paper works like a
charm in the \metaenv. Since the introduction of what has become known
as \textsc{apigen}, we have been able to effectuate considerable changes
in \asfix, and we have gained the ability to experiment with the format,
and quickly see the results working in our tools.

% }}}

% {{{  Future work

\section{Future work}
\label{sec:future}

The future work of this project falls into two categories: increasing
the efficiency of the generated code and generalizing our approach
to a wider application area.

Obvious optimizations include \emph{inlining} (some of) the generated
functions. By rewriting the functions as \C\ macros, the overhead of a
function call is removed. As noted before, the cost of this efficiency
gain is that some type-safety is lost. This is due to the fact that \C\
macro expansion is performed by a pre-compiler, which does not have
access to type information and thus performs no type checks on macro
arguments.  A typical approach would be to generate typesafe functions
in the development stage, and switch to the use of generated efficient
macros for production code. This approach is comparable to the use of
\texttt{assert} macro's that are completely eliminated in production
versions of the software.

More interesting, however, are optimizations that take into account
information about the structure of the underlying \aterm\ representation.
During the generation phase information about common subterms and
similarity between alternatives is assembled, which could be exploited
to generate more efficient matching and selection code than the current
\texttt{ATmatch} call, which is rather inefficient.

In a way the project described in this paper can be seen as a casestudy in
generative programming as described in Section~\ref{sec:related}. We
want to extend this casestudy into a more generic approach. This
approach will be based on a modular generic generator that takes a
set of abstract data definitions and generates code for them. The code
generator must be extendable with domain specific ``modules'' to generate
extra functionality. These modules should not only be able to add extra
functions when needed, but they should also be able to use \emph{Aspect
Oriented Programming}\cite{aspect} to add functionality to functions
that are generated by other modules.

For example, one of the more basic modules (the ``accessor'' module) could
generate the actual data representation (for instance simple attributes
in an object oriented setting) and accessors on this representation,
while another module could add transparent persistency using a standard
relational database by instrumenting all accessors.

The most important challenge in such an approach would be to create
an environment where the threshold to create new generator modules is
extremely low. In the ideal situation software developers could add new
modules to the generator just as easy as to add new modules directly to
the software system they are building.

% }}}

% {{{  Bibliography

\bibliographystyle{alpha}
\bibliography{apigen}

% }}}
% {{{  Appendix

\appendix

% {{{  Concrete Syntax of ATerms

\section{Concrete Syntax of \aterms}
\label{app:aterm-syntax}
A formal definition of the concrete syntax for \aterms\ using \sdf\
is presented here.

\begin{scriptsize}
\verbatiminput{ATerms.sdf}
\end{scriptsize}

% }}}
% {{{  Concrete Syntax of AsFix

\section{Concrete Syntax of \asfix}
\label{app:asfix-syntax}
A formal definition of the concrete syntax for \asfix\ using \sdf\
is presented here.

\begin{scriptsize}
\verbatiminput{Symbol.sdf}
\verbatiminput{Tree.sdf}
\verbatiminput{Attributes.sdf}
\end{scriptsize}

% }}}
% {{{  Example generated dictionary file

\section{Example generated dictionary file}
\label{app:bool_and_dict}
An abbreviated version of the generated dictionary file for the
parse tree syntax (\asfix) of the boolean \texttt{and}. Multiple
similar lines have been condensed (\texttt{...}).
\begin{scriptsize}
\verbatiminput{bool_and_dict}
\end{scriptsize}

% }}}
% }}}

\end{document}
